{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PW-0AFnooshd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ROG STRIX\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n0S2ZfOI4Sk-"
   },
   "outputs": [],
   "source": [
    "df_phone_dataset = pd.read_csv('phone_dataset.csv')\n",
    "PHONE_COUNT = len(df_phone_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dCK6HZ1n2q5I"
   },
   "outputs": [],
   "source": [
    "pd_1 = pd.read_csv('user_clicks.csv')\n",
    "pd_2 = pd.read_csv('user_ratings.csv')\n",
    "pd_3 = pd.read_csv('user_surveys.csv')\n",
    "\n",
    "MIN_USER_ID = min(pd_1['user_id'].min(), pd_2['user_id'].min(), pd_3['user_id'].min())\n",
    "MAX_USER_ID = max(pd_1['user_id'].max(), pd_2['user_id'].max(), pd_3['user_id'].max())\n",
    "TOTAL_USER = MAX_USER_ID - MIN_USER_ID + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc20oTgw20zt"
   },
   "source": [
    "##Preprocess Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkIE5M2opE-T"
   },
   "source": [
    "### Preprocess Dataset User Clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VrJpunjD2MZP",
    "outputId": "4f2095af-607f-41e0-f5a6-7e3f2ce1e9d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>phone_id</th>\n",
       "      <th>visit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2024-05-25 10:42:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-05-25 10:42:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-05-25 10:42:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2024-05-25 10:42:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2024-05-25 10:42:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  phone_id           visit_time\n",
       "0        1        15  2024-05-25 10:42:55\n",
       "1        1         2  2024-05-25 10:42:55\n",
       "2        1         3  2024-05-25 10:42:55\n",
       "3        1         7  2024-05-25 10:42:55\n",
       "4        1        14  2024-05-25 10:42:55"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_clicks = pd.read_csv('user_clicks.csv')\n",
    "df_user_clicks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hP2QiQ5h2mpm",
    "outputId": "56914dbc-4cda-4c85-f1e8-e5a33554bb6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>phone_id</th>\n",
       "      <th>visit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>58</td>\n",
       "      <td>2024-05-25 10:47:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "      <td>53</td>\n",
       "      <td>2024-05-25 10:47:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1500</td>\n",
       "      <td>21</td>\n",
       "      <td>2024-05-25 10:47:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>94</td>\n",
       "      <td>2024-05-25 10:47:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500</td>\n",
       "      <td>19</td>\n",
       "      <td>2024-05-25 10:47:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  phone_id           visit_time\n",
       "0     1500        58  2024-05-25 10:47:38\n",
       "1     1500        53  2024-05-25 10:47:38\n",
       "2     1500        21  2024-05-25 10:47:38\n",
       "3     1500        94  2024-05-25 10:47:38\n",
       "4     1500        19  2024-05-25 10:47:38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_clicks = df_user_clicks.sort_values(by='visit_time', ascending=False)\n",
    "df_user_clicks = df_user_clicks.drop_duplicates(subset=['user_id', 'phone_id'], keep='first')\n",
    "df_user_clicks = df_user_clicks.groupby('user_id').head(20)\n",
    "df_user_clicks = df_user_clicks.reset_index(drop=True)\n",
    "df_user_clicks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWNG5d4q3oU5",
    "outputId": "8ca32e43-b6c6-4fb5-dca1-8e02d782f8d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_counts = df_user_clicks['user_id'].value_counts()\n",
    "users_with_count_gt_20 = user_counts[user_counts > 20]\n",
    "len(users_with_count_gt_20.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qoiF8TTc55jv"
   },
   "outputs": [],
   "source": [
    "# Identify missing user IDs\n",
    "missing_user_ids = set(range(MIN_USER_ID, MAX_USER_ID + 1)) - set(df_user_clicks['user_id'])\n",
    "\n",
    "# Create a DataFrame with missing user IDs and clicks initialized to 0\n",
    "missing_data = pd.DataFrame({'user_id': list(missing_user_ids), 'phone_id': -1, 'visit_time': '2024-01-01 00:00:00'})\n",
    "\n",
    "# Append the missing data to the original DataFrame\n",
    "df_user_clicks_complete = pd.concat([df_user_clicks, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort by user_id to maintain order\n",
    "df_user_clicks_complete = df_user_clicks_complete.sort_values(by='user_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "THDME0Ou3CYd",
    "outputId": "7ee4cf79-13a4-4d2f-8edb-19a1148d24e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 1507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id   1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "phone_id                                                              ...   \n",
       "1            1     1     1     0     0     0     0     0     1     0  ...   \n",
       "2            0     0     0     0     0     1     0     0     0     1  ...   \n",
       "3            1     1     1     0     1     0     0     0     1     1  ...   \n",
       "4            1     0     0     0     0     0     1     0     0     0  ...   \n",
       "5            0     1     0     0     1     0     1     0     1     0  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "92           0     0     0     0     0     0     0     1     0     1  ...   \n",
       "93           0     0     0     0     0     0     1     0     1     0  ...   \n",
       "94           0     0     0     1     0     0     0     0     0     1  ...   \n",
       "95           0     0     0     1     0     0     0     1     0     1  ...   \n",
       "96           0     0     0     0     0     0     0     1     0     0  ...   \n",
       "\n",
       "user_id   1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  \n",
       "phone_id                                                              \n",
       "1            0     0     0     0     0     0     0     0     0     0  \n",
       "2            0     0     0     0     0     0     0     0     0     0  \n",
       "3            0     0     0     0     0     0     0     0     0     0  \n",
       "4            0     0     0     0     0     0     0     0     0     0  \n",
       "5            1     0     0     0     0     0     0     0     0     0  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "92           0     0     0     0     0     0     0     0     0     0  \n",
       "93           0     0     0     0     0     0     0     0     0     0  \n",
       "94           0     0     1     0     0     0     0     0     0     0  \n",
       "95           0     0     0     0     0     0     0     0     0     0  \n",
       "96           0     1     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[96 rows x 1507 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clicks = df_user_clicks_complete[['user_id', 'phone_id']].pivot_table(index='phone_id', columns='user_id', aggfunc=lambda x: 1 if len(x) > 0 else 0, fill_value=0)\n",
    "train_clicks = train_clicks[1:]\n",
    "train_clicks.head(96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IEdNVI4J3xi"
   },
   "source": [
    "###Preprocess Dataset User Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "A_zlx_1eJ-n6",
    "outputId": "caf86a8e-8f4d-4585-cbfa-19dcece29e2e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>phone_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  phone_id  rating\n",
       "0        1        15     4.0\n",
       "1        1         2     5.0\n",
       "2        1         3     5.0\n",
       "3        1         7     5.0\n",
       "4        1        14     4.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_ratings = pd.read_csv(\"user_ratings.csv\")\n",
    "df_user_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zNPkVynMCNz",
    "outputId": "532c8e74-d485-4c26-e8b7-9e5c08bee4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_ratings.drop_duplicates(inplace=True)\n",
    "df_user_ratings.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhMn-XV4RiyF",
    "outputId": "13304105-b8a1-4053-9da7-61fa1e2a0588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10672 entries, 0 to 10671\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   user_id   10672 non-null  int64  \n",
      " 1   phone_id  10672 non-null  int64  \n",
      " 2   rating    10439 non-null  float64\n",
      "dtypes: float64(1), int64(2)\n",
      "memory usage: 250.3 KB\n"
     ]
    }
   ],
   "source": [
    "df_user_ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "tB7VRKjD7Lxy"
   },
   "outputs": [],
   "source": [
    "# Identify missing user IDs\n",
    "missing_user_ids = set(range(MIN_USER_ID, MAX_USER_ID + 1)) - set(df_user_ratings['user_id'])\n",
    "\n",
    "# Create a DataFrame with missing user IDs and clicks initialized to 0\n",
    "missing_data = pd.DataFrame({'user_id': list(missing_user_ids), 'phone_id': 1, 'rating': 0})\n",
    "missing_data['user_id'] = missing_data['user_id'].astype(int)\n",
    "# Append the missing data to the original DataFrame\n",
    "df_user_ratings_complete = pd.concat([df_user_ratings, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort by user_id to maintain order\n",
    "df_user_ratings_complete = df_user_ratings_complete.sort_values(by='user_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "5sB948YiKc2r",
    "outputId": "e183e8d3-fe67-4e80-9380-d0aeb2cd431c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 1507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id   1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "phone_id                                                              ...   \n",
       "1            5     5     5     0     5     5     0     0     4     0  ...   \n",
       "2            5     0     0     5     0     4     4     0     0     5  ...   \n",
       "3            5     5     0     4     0     5     0     0     4     0  ...   \n",
       "4            5     0     0     0     0     4     4     0     0     0  ...   \n",
       "5            0     5     0     5     4     0     4     0     5     4  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "92           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "93           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "94           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "95           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "96           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "user_id   1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  \n",
       "phone_id                                                              \n",
       "1            0     0     0     3     3     3     3     5     0     0  \n",
       "2            0     0     0     3     3     3     3     5     0     5  \n",
       "3            0     0     0     3     3     3     3     4     0     3  \n",
       "4            0     0     0     3     4     3     3     5     0     5  \n",
       "5            0     0     0     4     3     3     3     5     0     3  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "92           0     0     0     4     3     3     3     0     0     0  \n",
       "93           0     0     0     3     5     5     4     3     0     0  \n",
       "94           0     0     0     5     5     5     4     5     0     0  \n",
       "95           0     0     0     4     4     5     4     3     0     0  \n",
       "96           0     0     0     4     3     5     4     5     0     0  \n",
       "\n",
       "[96 rows x 1507 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ratings = df_user_ratings_complete.pivot(index='phone_id', columns='user_id', values='rating').fillna(0)\n",
    "train_ratings = train_ratings.astype(int)\n",
    "train_ratings.head(PHONE_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJNwcLZ4pASL"
   },
   "source": [
    "### Preprocess Dataset Surveys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yWDZdkKapLeU",
    "outputId": "250e33a7-d443-4c9a-bb8e-d297191c88b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>q4</th>\n",
       "      <th>q5</th>\n",
       "      <th>q6</th>\n",
       "      <th>q7</th>\n",
       "      <th>q8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>503</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>504</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>505</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  q1  q2  q3  q4  q5  q6    q7  q8\n",
       "0      501   3   5   3   1   3   1  True   6\n",
       "1      502   2   4   2   5   2   1  True   2\n",
       "2      503   5   4   4   5   4   5  True   8\n",
       "3      504   5   4   3   2   5   2  True   2\n",
       "4      505   5   1   2   1   5   5  True   5"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_surveys = pd.read_csv('user_surveys.csv')\n",
    "df_user_surveys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "_T4DIgG6CQji"
   },
   "outputs": [],
   "source": [
    "df_user_surveys['q7'] = df_user_surveys['q7'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "Ikx0-3os8GaT"
   },
   "outputs": [],
   "source": [
    "# Identify missing user IDs\n",
    "missing_user_ids = set(range(MIN_USER_ID, MAX_USER_ID + 1)) - set(df_user_surveys['user_id'])\n",
    "\n",
    "# Create a DataFrame with missing user IDs and clicks initialized to 0\n",
    "missing_data = pd.DataFrame({'user_id': list(missing_user_ids), 'q1': 0, 'q2': 0, 'q3': 0, 'q4': 0, 'q5': 0, 'q6': 0, 'q7': 0, 'q8': -1})\n",
    "missing_data['user_id'] = missing_data['user_id'].astype(int)\n",
    "# Append the missing data to the original DataFrame\n",
    "df_user_surveys_complete = pd.concat([df_user_surveys, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort by user_id to maintain order\n",
    "df_user_surveys_complete = df_user_surveys_complete.sort_values(by='user_id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "a1IfFhFTrPC1",
    "outputId": "665a25de-9c0d-488e-8fcf-59f26c214358"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id  1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "q1          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q2          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q3          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q4          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q5          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q6          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q7          0     0     0     0     0     0     0     0     0     0  ...   \n",
       "q8         -1    -1    -1    -1    -1    -1    -1    -1    -1    -1  ...   \n",
       "\n",
       "user_id  1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  \n",
       "q1          2     3     1     4     3     5     5     5     5     5  \n",
       "q2          2     4     4     5     4     4     3     3     3     5  \n",
       "q3          3     2     2     5     5     5     5     5     5     5  \n",
       "q4          3     4     5     4     3     3     5     5     5     3  \n",
       "q5          5     4     5     5     4     4     5     5     5     5  \n",
       "q6          2     5     3     5     3     5     5     5     5     3  \n",
       "q7          1     1     1     1     1     1     1     1     0     1  \n",
       "q8          1     0     8     3     4     8     3     8    -1     3  \n",
       "\n",
       "[8 rows x 1507 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_surveys =  df_user_surveys_complete.set_index('user_id').T\n",
    "train_surveys.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQuUNEre1yQo"
   },
   "source": [
    "##Collaborative filtering learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjzQyQNi16W3"
   },
   "source": [
    "### Collaborative filtering cost (eksperimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "8mXleHBmIVxe"
   },
   "outputs": [],
   "source": [
    "input_1 = tf.convert_to_tensor(train_ratings.values)\n",
    "input_2 = tf.convert_to_tensor(train_clicks.values)\n",
    "input_2 = tf.cast(input_2, tf.float32)\n",
    "input_3 = tf.convert_to_tensor(train_surveys.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xOTtGlng1-5n"
   },
   "outputs": [],
   "source": [
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ac0oTEVk8rWV"
   },
   "outputs": [],
   "source": [
    "# def collaborative_model():\n",
    "#   input_1 = tf.keras.layers.Input(shape=[PHONE_COUNT, TOTAL_USER])\n",
    "#   input_2 = tf.keras.layers.Input(shape=[PHONE_COUNT, TOTAL_USER])\n",
    "#   input_3 = tf.keras.layers.Input(shape=[8, TOTAL_USER])\n",
    "\n",
    "#   input_1 = tf.keras.layers.Flatten()(input_1)\n",
    "#   input_2 = tf.keras.layers.Flatten()(input_2)\n",
    "#   input_3 = tf.keras.layers.Flatten()(input_3)\n",
    "\n",
    "#   concat_1 = tf.keras.layers.concatenate([input_1, input_2])\n",
    "#   x_1 = tf.keras.layers.Dense(units=128, activation='relu')(concat_1)\n",
    "\n",
    "#   x_2 = tf.keras.layers.Dense(units=128, activation='relu')(input_3)\n",
    "\n",
    "#   concat_2 = tf.keras.layers.concatenate([x_1, x_2])\n",
    "#   output = tf.keras.layers.Dense(units=1, activation='sigmoid')(concat_2)\n",
    "\n",
    "#   return tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=[output])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "v1N33NIvFV0E"
   },
   "outputs": [],
   "source": [
    "def collaborative_model():\n",
    "  input_1 = tf.keras.layers.Input(shape=[PHONE_COUNT, TOTAL_USER])\n",
    "  input_2 = tf.keras.layers.Input(shape=[PHONE_COUNT, TOTAL_USER])\n",
    "\n",
    "\n",
    "  input_3 = tf.keras.layers.Input(shape=[8, TOTAL_USER])\n",
    "\n",
    "  input_1 = tf.keras.layers.Flatten()(input_1)\n",
    "  input_2 = tf.keras.layers.Flatten()(input_2)\n",
    "  input_3 = tf.keras.layers.Flatten()(input_3)\n",
    "\n",
    "  concat = tf.keras.layers.concatenate([input_1, input_2, input_3])\n",
    "  x = tf.keras.layers.Dense(units=128, activation='relu')(concat)\n",
    "\n",
    "  output = tf.keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "sBGUuG6wAF-5",
    "outputId": "8c54baa4-aa39-4f7e-d22a-f95ceaa59521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ROG STRIX\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = collaborative_model()\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9ZOGWPr3oUr"
   },
   "source": [
    "### Collaborative Filtering for User Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "0C9WkbdO3qv8",
    "outputId": "81147fba-c040-4dc3-d146-09fc5e09fcd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG STRIX\\AppData\\Local\\Temp\\ipykernel_39668\\1514983918.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  R_train_ratings = train_ratings.applymap(lambda x: 1 if x != 0 else 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>user_id</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 1507 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "user_id   1     2     3     4     5     6     7     8     9     10    ...  \\\n",
       "phone_id                                                              ...   \n",
       "1            1     1     1     0     1     1     0     0     1     0  ...   \n",
       "2            1     0     0     1     0     1     1     0     0     1  ...   \n",
       "3            1     1     0     1     0     1     0     0     1     0  ...   \n",
       "4            1     0     0     0     0     1     1     0     0     0  ...   \n",
       "5            0     1     0     1     1     0     1     0     1     1  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "92           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "93           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "94           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "95           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "96           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "user_id   1498  1499  1500  1501  1502  1503  1504  1505  1506  1507  \n",
       "phone_id                                                              \n",
       "1            0     0     0     1     1     1     1     1     0     0  \n",
       "2            0     0     0     1     1     1     1     1     0     1  \n",
       "3            0     0     0     1     1     1     1     1     0     1  \n",
       "4            0     0     0     1     1     1     1     1     0     1  \n",
       "5            0     0     0     1     1     1     1     1     0     1  \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "92           0     0     0     1     1     1     1     0     0     0  \n",
       "93           0     0     0     1     1     1     1     1     0     0  \n",
       "94           0     0     0     1     1     1     1     1     0     0  \n",
       "95           0     0     0     1     1     1     1     1     0     0  \n",
       "96           0     0     0     1     1     1     1     1     0     0  \n",
       "\n",
       "[96 rows x 1507 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_train_ratings = train_ratings.applymap(lambda x: 1 if x != 0 else 0)\n",
    "R_train_ratings.head(PHONE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "q2idj_IRUTZp"
   },
   "outputs": [],
   "source": [
    "def cost_func_user_ratings(X, W, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y) * R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "k3bypsDeX-1Z"
   },
   "outputs": [],
   "source": [
    "class CollaborativeFilteringModel:\n",
    "    def __init__(self, num_users, num_items, num_features, lambda_=0.1, learning_rate=0.01):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.lambda_ = lambda_\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.X = tf.Variable(tf.random.normal([num_items, num_features]), dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random.normal([num_users, num_features]), dtype=tf.float32)\n",
    "        self.b = tf.Variable(tf.zeros([num_items, num_users]), dtype=tf.float32)\n",
    "\n",
    "    def cost_func_user_ratings(self, X, W, b, Y, R):\n",
    "        j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y) * R\n",
    "        J = 0.5 * tf.reduce_sum(j**2) + (self.lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "        return J\n",
    "\n",
    "    def train(self, Y, R, epochs=1000):\n",
    "        optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                cost = self.cost_func_user_ratings(self.X, self.W, self.b, Y, R)\n",
    "\n",
    "            grads = tape.gradient(cost, [self.X, self.W, self.b])\n",
    "            optimizer.apply_gradients(zip(grads, [self.X, self.W, self.b]))\n",
    "\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Cost: {cost.numpy()}\")\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.linalg.matmul(self.X, tf.transpose(self.W)) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "MjRg1682afdA",
    "outputId": "a8e451f0-5255-4c3b-ecd5-64e9df180f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/2000, Cost: 7547.83251953125\n",
      "Epoch 200/2000, Cost: 6940.7177734375\n",
      "Epoch 300/2000, Cost: 6342.85107421875\n",
      "Epoch 400/2000, Cost: 5760.556640625\n",
      "Epoch 500/2000, Cost: 5213.9189453125\n",
      "Epoch 600/2000, Cost: 4709.837890625\n",
      "Epoch 700/2000, Cost: 4249.34716796875\n",
      "Epoch 800/2000, Cost: 3830.875\n",
      "Epoch 900/2000, Cost: 3451.759521484375\n",
      "Epoch 1000/2000, Cost: 3108.968017578125\n",
      "Epoch 1100/2000, Cost: 2799.433837890625\n",
      "Epoch 1200/2000, Cost: 2520.2109375\n",
      "Epoch 1300/2000, Cost: 2268.541748046875\n",
      "Epoch 1400/2000, Cost: 2041.8782958984375\n",
      "Epoch 1500/2000, Cost: 1837.8837890625\n",
      "Epoch 1600/2000, Cost: 1654.425537109375\n",
      "Epoch 1700/2000, Cost: 1489.560302734375\n",
      "Epoch 1800/2000, Cost: 1341.521728515625\n",
      "Epoch 1900/2000, Cost: 1208.703857421875\n",
      "Epoch 2000/2000, Cost: 1089.6473388671875\n"
     ]
    }
   ],
   "source": [
    "model = CollaborativeFilteringModel(num_users = TOTAL_USER, num_items = PHONE_COUNT, num_features = 100)\n",
    "\n",
    "model.train(Y = train_ratings, R = R_train_ratings, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "cAlFp5jRZGWQ"
   },
   "outputs": [],
   "source": [
    "# Collaborative Filtering Model\n",
    "class CollaborativeFilteringModels(tf.Module):\n",
    "    def __init__(self, num_users, num_items, num_features, lambda_):\n",
    "        # Initialize model parameters\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        # Initialize user and item feature matrices with random values\n",
    "        self.X = tf.Variable(tf.random.normal([num_items, num_features]), name='X')\n",
    "        self.W = tf.Variable(tf.random.normal([num_users, num_features]), name='W')\n",
    "\n",
    "        # Initialize bias terms\n",
    "        self.b = tf.Variable(tf.random.normal([num_items, num_users]), name='b')\n",
    "\n",
    "    def __call__(self, Y, R):\n",
    "        return cost_func_user_ratings(self.X, self.W, self.b, Y, R, self.lambda_)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, Y, R, learning_rate, num_epochs):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = model(Y, R)\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(cost, [model.X, model.W, model.b])\n",
    "\n",
    "        # Apply gradients to optimize the parameters\n",
    "        optimizer.apply_gradients(zip(gradients, [model.X, model.W, model.b]))\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoch {epoch + 1}, Cost: {cost.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Nt8mkfdrakgw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m CollaborativeFilteringModels(num_users \u001b[38;5;241m=\u001b[39m TOTAL_USER, num_items \u001b[38;5;241m=\u001b[39m PHONE_COUNT, num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_model(model, \u001b[43mY\u001b[49m, R, learning_rate, num_epochs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "model = CollaborativeFilteringModels(num_users = TOTAL_USER, num_items = PHONE_COUNT, num_features = 100, lambda_ = 0.1)\n",
    "\n",
    "train_model(model, Y, R, learning_rate, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyKk9AbyWdrB"
   },
   "outputs": [],
   "source": [
    "def collaborative_filtering_model(num_users, num_items, num_features, lambda_, learning_rate=0.001, num_epochs=1000):\n",
    "    # Initialize the variables\n",
    "    X = tf.Variable(tf.random.normal([num_items, num_features], stddev=0.35), name='X')\n",
    "    W = tf.Variable(tf.random.normal([num_users, num_features], stddev=0.35), name='W')\n",
    "    b = tf.Variable(tf.random.normal([num_items, num_users], stddev=0.35), name='b')\n",
    "\n",
    "    # Placeholder for the true ratings and the R matrix\n",
    "    Y = tf.placeholder(tf.float32, shape=[num_items, num_users], name='Y')\n",
    "    R = tf.placeholder(tf.float32, shape=[num_items, num_users], name='R')\n",
    "\n",
    "    # Cost function\n",
    "    def cost_func_user_ratings(X, W, b, Y, R, lambda_):\n",
    "        j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y) * R\n",
    "        J = 0.5 * tf.reduce_sum(tf.square(j)) + (lambda_/2) * (tf.reduce_sum(tf.square(X)) + tf.reduce_sum(tf.square(W)))\n",
    "        return J\n",
    "\n",
    "    # Define the cost function\n",
    "    cost = cost_func_user_ratings(X, W, b, Y, R, lambda_)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Function to train the model\n",
    "    def train_model(Y_train, R_train, num_epochs=num_epochs):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            for epoch in range(num_epochs):\n",
    "                _, current_cost = sess.run([optimizer, cost], feed_dict={Y: Y_train, R: R_train})\n",
    "                if epoch % 100 == 0:\n",
    "                    print(f\"Epoch {epoch}, Cost: {current_cost}\")\n",
    "\n",
    "            # Get the trained values of X, W, and b\n",
    "            trained_X = sess.run(X)\n",
    "            trained_W = sess.run(W)\n",
    "            trained_b = sess.run(b)\n",
    "\n",
    "        return trained_X, trained_W, trained_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2T6OPkoZWhFI"
   },
   "outputs": [],
   "source": [
    "model = collaborative_filtering_model(TOTAL_USER, PHONE_COUNT, 100, 1, learning_rate=0.001, num_epochs=1000)\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AkGe5C5UdGZ"
   },
   "outputs": [],
   "source": [
    "def collaborative_model_ratings(num_features):\n",
    "    W = tf.keras.layers.Embedding(input_dim=TOTAL_USER, output_dim=num_features, embeddings_initializer='random_normal', name='W')\n",
    "    X = tf.keras.layers.Embedding(input_dim=PHONE_COUNT, output_dim=num_features, embeddings_initializer='random_normal', name='X')\n",
    "    b = tf.keras.layers.Embedding(input_dim=TOTAL_USER, output_dim=1, embeddings_initializer='random_normal', name='b')\n",
    "\n",
    "    user_input = tf.keras.Input(shape=(TOTAL_USER,), dtype=tf.int32, name='user_input')\n",
    "    phone_input = tf.keras.Input(shape=(PHONE_COUNT,), dtype=tf.int32, name='phone_input')\n",
    "\n",
    "    user_embedding = W(user_input)\n",
    "    phone_embedding = X(phone_input)\n",
    "    user_bias = b(user_input)\n",
    "\n",
    "    dot_product = tf.keras.layers.Dot(axes=-1)([user_embedding, phone_embedding])\n",
    "    output = tf.keras.layers.Add()([dot_product, user_bias])\n",
    "    output = tf.keras.layers.Flatten()(output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[user_input, phone_input], outputs=output, name='collaborative_model_ratings')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPHGBkJlVw7B"
   },
   "outputs": [],
   "source": [
    "model = collaborative_model_ratings(26)\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpL00zRd3rWh"
   },
   "source": [
    "### Collaborative  Filtering for User Click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "B20eSpNH3t31"
   },
   "outputs": [],
   "source": [
    "def cost_func_user_clicks(X, W, b, Y, lambda_):\n",
    "    # j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)\n",
    "    b = tf.reshape(b, (TOTAL_USER))\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)))\n",
    "    j = j + b - Y\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "LKFkuSsy37CZ"
   },
   "outputs": [],
   "source": [
    "def collaborative_model_clicks(num_features):\n",
    "    W = tf.keras.layers.Embedding(input_dim=TOTAL_USER, output_dim=num_features, embeddings_initializer='random_normal', name='W')\n",
    "    X = tf.keras.layers.Embedding(input_dim=PHONE_COUNT, output_dim=num_features, embeddings_initializer='random_normal', name='X')\n",
    "    b = tf.keras.layers.Embedding(input_dim=TOTAL_USER, output_dim=1, embeddings_initializer='random_normal', name='b')\n",
    "\n",
    "    user_input = tf.keras.Input(shape=(TOTAL_USER,), dtype=tf.int32, name='user_input')\n",
    "    phone_input = tf.keras.Input(shape=(PHONE_COUNT,), dtype=tf.int32, name='phone_input')\n",
    "\n",
    "    user_embedding = W(user_input)\n",
    "    phone_embedding = X(phone_input)\n",
    "    user_bias = b(user_input)\n",
    "\n",
    "    dot_product = tf.keras.layers.Dot(axes=-1)([user_embedding, phone_embedding])\n",
    "    output = tf.keras.layers.Add()([dot_product, user_bias])\n",
    "    output = tf.keras.layers.Flatten()(output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[user_input, phone_input], outputs=output, name='collaborative_model_clicks')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "OJq8y-q-6Cxs",
    "outputId": "122b6db1-ea20-499f-f2c6-99ac39c896e7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = collaborative_model_clicks(100)\n",
    "tf.keras.utils.plot_model(model)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ZQ3WF_K6mDL",
    "outputId": "83e681fe-c367-426a-b08c-2c61620b5a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 14473.1572\n",
      "Training loss at iteration 20: 6220.5972\n",
      "Training loss at iteration 40: 2918.3303\n",
      "Training loss at iteration 60: 1827.0730\n",
      "Training loss at iteration 80: 1510.1418\n",
      "Training loss at iteration 100: 1428.1083\n",
      "Training loss at iteration 120: 1400.5841\n",
      "Training loss at iteration 140: 1389.3308\n",
      "Training loss at iteration 160: 1383.6011\n",
      "Training loss at iteration 180: 1380.1638\n",
      "Training loss at iteration 200: 1377.9044\n",
      "Training loss at iteration 220: 1376.3915\n",
      "Training loss at iteration 240: 1375.3838\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "iterations = 250\n",
    "lambda_ = 1\n",
    "\n",
    "# Convert model layers to variables for custom training\n",
    "W_var = model.get_layer('W').embeddings\n",
    "X_var = model.get_layer('X').embeddings\n",
    "b_var = model.get_layer('b').embeddings\n",
    "\n",
    "for iter in range(iterations):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost_value = cost_func_user_clicks(X_var, W_var, b_var, input_2, lambda_)\n",
    "\n",
    "    grads = tape.gradient(cost_value, [W_var, X_var, b_var])\n",
    "    optimizer.apply_gradients(zip(grads, [W_var, X_var, b_var]))\n",
    "\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value.numpy():0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "u-GtVjJoAIfk"
   },
   "outputs": [],
   "source": [
    "W_var = model.get_layer('W').embeddings\n",
    "X_var = model.get_layer('X').embeddings\n",
    "b_var = model.get_layer('b').embeddings\n",
    "\n",
    "p = np.matmul(X_var.numpy(), np.transpose(W_var.numpy())) + np.reshape(b_var.numpy(), (TOTAL_USER))\n",
    "\n",
    "user_id = 155\n",
    "\n",
    "probability = p[:, user_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probability clicking 0.95 for phone Oppo Reno 11 Pro id 1\n",
      "2. Probability clicking 0.95 for phone Infinix Hot 40 Pro id 26\n",
      "3. Probability clicking 0.94 for phone Infinix Zero 30 5G id 27\n",
      "4. Probability clicking 0.95 for phone Samsung Galaxy A15 id 29\n",
      "5. Probability clicking 0.95 for phone Samsung Galaxy A25 id 30\n",
      "6. Probability clicking 0.94 for phone Samsung Galaxy Z Fold5 id 34\n",
      "7. Probability clicking 0.95 for phone Samsung Galaxy Z Flip5 id 35\n",
      "8. Probability clicking 0.94 for phone Samsung Galaxy M34 id 36\n",
      "9. Probability clicking 0.95 for phone Vivo Y17s id 49\n",
      "10. Probability clicking 0.95 for phone Huawei P60 Pro id 56\n",
      "11. Probability clicking 0.95 for phone Huawei Nova Y72 id 58\n",
      "12. Probability clicking 0.95 for phone iPhone 14 id 64\n",
      "13. Probability clicking 0.95 for phone iPhone 14 Pro id 66\n",
      "14. Probability clicking 0.95 for phone iPhone 15 Pro Max id 71\n",
      "15. Probability clicking 0.95 for phone Realme C53 id 77\n",
      "16. Probability clicking 0.95 for phone Realme 12 5G id 81\n",
      "17. Probability clicking 0.94 for phone Redmi Note 13 Pro 4G id 87\n",
      "18. Probability clicking 0.95 for phone Poco F5 id 93\n",
      "19. Probability clicking 0.95 for phone Poco X6 Pro 5G id 94\n",
      "20. Probability clicking 0.96 for phone Poco X6 5G id 95\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "for i in range(PHONE_COUNT):\n",
    "    if probability[i] > 0.5:\n",
    "        name =  df_phone_dataset.iloc[i]['name']\n",
    "        id =  df_phone_dataset.iloc[i]['id']\n",
    "        print(f'{index}. Probability clicking {probability[i]:0.2f} for phone { name } id { id }')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['probability'] = probability\n",
    "df_result['name'] = df_phone_dataset['name']\n",
    "sorted_df_result = df_result.sort_values(by='probability', ascending=False)\n",
    "sorted_df_result = sorted_df_result[sorted_df_result['probability'] <= 0.5]\n",
    "sorted_df_result['probability'] = sorted_df_result['probability'] / sorted_df_result['probability'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probability clicking 1.00 for phone Samsung Galaxy M54\n",
      "2. Probability clicking 0.94 for phone Infinix GT 20 Pro\n",
      "3. Probability clicking 0.90 for phone Samsung Galaxy S24 Ultra\n",
      "4. Probability clicking 0.88 for phone Huawei Pura 70 Pro\n",
      "5. Probability clicking 0.85 for phone iPhone 15\n",
      "6. Probability clicking 0.82 for phone Vivo T3x\n",
      "7. Probability clicking 0.80 for phone Infinix Note 40 Pro\n",
      "8. Probability clicking 0.79 for phone Samsung Galaxy A55\n",
      "9. Probability clicking 0.78 for phone Asus Zenfone 10\n",
      "10. Probability clicking 0.78 for phone Huawei Mate X3\n"
     ]
    }
   ],
   "source": [
    "datas = sorted_df_result.to_numpy()\n",
    "for i in range(10):\n",
    "    prob = datas[i][0]\n",
    "    print(f'{i + 1}. Probability clicking {prob:0.2f} for phone { (datas[i][1]) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collaborative_model_clicks_functional(num_features):\n",
    "    # X = tf.keras.layers.Embedding(input_dim=PHONE_COUNT, output_dim=num_features, embeddings_initializer='random_normal', name='X')\n",
    "    X = tf.keras.layers.Dense(units=num_features, activation=None, kernel_initializer='random_normal', name='X')\n",
    "    phone_input = tf.keras.Input(shape=(PHONE_COUNT,), dtype=tf.int32, name='phone_input')\n",
    "\n",
    " \n",
    "    phone_embedding = X(phone_input)\n",
    "\n",
    "    output = tf.keras.layers.Dense(PHONE_COUNT, activation='sigmoid')(phone_embedding)\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.Model(inputs=phone_input, outputs=output, name='collaborative_model_clicks')\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1507, 96)\n",
      "Epoch 1/1000\n",
      "48/48 [==============================] - 0s 853us/step - loss: 0.1840\n",
      "Epoch 2/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 0.1422\n",
      "Epoch 3/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.1291\n",
      "Epoch 4/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.1148\n",
      "Epoch 5/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 0.1004\n",
      "Epoch 6/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0862\n",
      "Epoch 7/1000\n",
      "48/48 [==============================] - 0s 794us/step - loss: 0.0731\n",
      "Epoch 8/1000\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.0614\n",
      "Epoch 9/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0513\n",
      "Epoch 10/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.0427\n",
      "Epoch 11/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.0356\n",
      "Epoch 12/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0296\n",
      "Epoch 13/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.0248\n",
      "Epoch 14/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.0208\n",
      "Epoch 15/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 0.0175\n",
      "Epoch 16/1000\n",
      "48/48 [==============================] - 0s 783us/step - loss: 0.0148\n",
      "Epoch 17/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0127\n",
      "Epoch 18/1000\n",
      "48/48 [==============================] - 0s 763us/step - loss: 0.0108\n",
      "Epoch 19/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 0.0094\n",
      "Epoch 20/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.0081\n",
      "Epoch 21/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 0.0071\n",
      "Epoch 22/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 0.0062\n",
      "Epoch 23/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 0.0055\n",
      "Epoch 24/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0048\n",
      "Epoch 25/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 0.0043\n",
      "Epoch 26/1000\n",
      "48/48 [==============================] - 0s 749us/step - loss: 0.0039\n",
      "Epoch 27/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 0.0035\n",
      "Epoch 28/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 0.0031\n",
      "Epoch 29/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0028\n",
      "Epoch 30/1000\n",
      "48/48 [==============================] - 0s 781us/step - loss: 0.0026\n",
      "Epoch 31/1000\n",
      "48/48 [==============================] - 0s 747us/step - loss: 0.0023\n",
      "Epoch 32/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 0.0021\n",
      "Epoch 33/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 0.0020\n",
      "Epoch 34/1000\n",
      "48/48 [==============================] - 0s 786us/step - loss: 0.0018\n",
      "Epoch 35/1000\n",
      "48/48 [==============================] - 0s 762us/step - loss: 0.0017\n",
      "Epoch 36/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 0.0015\n",
      "Epoch 37/1000\n",
      "48/48 [==============================] - 0s 815us/step - loss: 0.0014\n",
      "Epoch 38/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 0.0013\n",
      "Epoch 39/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 40/1000\n",
      "48/48 [==============================] - 0s 805us/step - loss: 0.0011\n",
      "Epoch 41/1000\n",
      "48/48 [==============================] - 0s 780us/step - loss: 0.0011\n",
      "Epoch 42/1000\n",
      "48/48 [==============================] - 0s 781us/step - loss: 9.9126e-04\n",
      "Epoch 43/1000\n",
      "48/48 [==============================] - 0s 773us/step - loss: 9.2715e-04\n",
      "Epoch 44/1000\n",
      "48/48 [==============================] - 0s 760us/step - loss: 8.6700e-04\n",
      "Epoch 45/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 8.1295e-04\n",
      "Epoch 46/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 7.6283e-04\n",
      "Epoch 47/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 7.1627e-04\n",
      "Epoch 48/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 6.7385e-04\n",
      "Epoch 49/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 6.3379e-04\n",
      "Epoch 50/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.9710e-04\n",
      "Epoch 51/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 5.6259e-04\n",
      "Epoch 52/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.3138e-04\n",
      "Epoch 53/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.0194e-04\n",
      "Epoch 54/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 4.7443e-04\n",
      "Epoch 55/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 4.4879e-04\n",
      "Epoch 56/1000\n",
      "48/48 [==============================] - 0s 818us/step - loss: 4.2515e-04\n",
      "Epoch 57/1000\n",
      "48/48 [==============================] - 0s 784us/step - loss: 4.0249e-04\n",
      "Epoch 58/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 3.8190e-04\n",
      "Epoch 59/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 3.6246e-04\n",
      "Epoch 60/1000\n",
      "48/48 [==============================] - 0s 761us/step - loss: 3.4434e-04\n",
      "Epoch 61/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.2728e-04\n",
      "Epoch 62/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.1115e-04\n",
      "Epoch 63/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 2.9608e-04\n",
      "Epoch 64/1000\n",
      "48/48 [==============================] - 0s 753us/step - loss: 2.8181e-04\n",
      "Epoch 65/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.6849e-04\n",
      "Epoch 66/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.5611e-04\n",
      "Epoch 67/1000\n",
      "48/48 [==============================] - 0s 764us/step - loss: 2.4404e-04\n",
      "Epoch 68/1000\n",
      "48/48 [==============================] - 0s 747us/step - loss: 2.3290e-04\n",
      "Epoch 69/1000\n",
      "48/48 [==============================] - 0s 782us/step - loss: 2.2246e-04\n",
      "Epoch 70/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2.1241e-04\n",
      "Epoch 71/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.0284e-04\n",
      "Epoch 72/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 1.9410e-04\n",
      "Epoch 73/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.8555e-04\n",
      "Epoch 74/1000\n",
      "48/48 [==============================] - 0s 751us/step - loss: 1.7770e-04\n",
      "Epoch 75/1000\n",
      "48/48 [==============================] - 0s 789us/step - loss: 1.6995e-04\n",
      "Epoch 76/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1.6289e-04\n",
      "Epoch 77/1000\n",
      "48/48 [==============================] - 0s 758us/step - loss: 1.5604e-04\n",
      "Epoch 78/1000\n",
      "48/48 [==============================] - 0s 807us/step - loss: 1.4965e-04\n",
      "Epoch 79/1000\n",
      "48/48 [==============================] - 0s 781us/step - loss: 1.4345e-04\n",
      "Epoch 80/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.3765e-04\n",
      "Epoch 81/1000\n",
      "48/48 [==============================] - 0s 765us/step - loss: 1.3119e-04\n",
      "Epoch 82/1000\n",
      "48/48 [==============================] - 0s 759us/step - loss: 1.2554e-04\n",
      "Epoch 83/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2068e-04\n",
      "Epoch 84/1000\n",
      "48/48 [==============================] - 0s 780us/step - loss: 1.1598e-04\n",
      "Epoch 85/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.1163e-04\n",
      "Epoch 86/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.0729e-04\n",
      "Epoch 87/1000\n",
      "48/48 [==============================] - 0s 804us/step - loss: 1.0314e-04\n",
      "Epoch 88/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 9.9361e-05\n",
      "Epoch 89/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.5596e-05\n",
      "Epoch 90/1000\n",
      "48/48 [==============================] - 0s 780us/step - loss: 9.2022e-05\n",
      "Epoch 91/1000\n",
      "48/48 [==============================] - 0s 786us/step - loss: 8.8621e-05\n",
      "Epoch 92/1000\n",
      "48/48 [==============================] - 0s 782us/step - loss: 8.5355e-05\n",
      "Epoch 93/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 8.2323e-05\n",
      "Epoch 94/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 7.9280e-05\n",
      "Epoch 95/1000\n",
      "48/48 [==============================] - 0s 760us/step - loss: 7.6438e-05\n",
      "Epoch 96/1000\n",
      "48/48 [==============================] - 0s 765us/step - loss: 7.3720e-05\n",
      "Epoch 97/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 7.1141e-05\n",
      "Epoch 98/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.8623e-05\n",
      "Epoch 99/1000\n",
      "48/48 [==============================] - 0s 786us/step - loss: 6.6216e-05\n",
      "Epoch 100/1000\n",
      "48/48 [==============================] - 0s 753us/step - loss: 6.3892e-05\n",
      "Epoch 101/1000\n",
      "48/48 [==============================] - 0s 786us/step - loss: 6.1677e-05\n",
      "Epoch 102/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 5.9553e-05\n",
      "Epoch 103/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.7523e-05\n",
      "Epoch 104/1000\n",
      "48/48 [==============================] - 0s 731us/step - loss: 5.5580e-05\n",
      "Epoch 105/1000\n",
      "48/48 [==============================] - 0s 805us/step - loss: 5.3690e-05\n",
      "Epoch 106/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 5.1900e-05\n",
      "Epoch 107/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 5.0149e-05\n",
      "Epoch 108/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 4.8478e-05\n",
      "Epoch 109/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 4.6884e-05\n",
      "Epoch 110/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 4.5327e-05\n",
      "Epoch 111/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 4.3843e-05\n",
      "Epoch 112/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 4.2406e-05\n",
      "Epoch 113/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 4.1019e-05\n",
      "Epoch 114/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.9685e-05\n",
      "Epoch 115/1000\n",
      "48/48 [==============================] - 0s 773us/step - loss: 3.8413e-05\n",
      "Epoch 116/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.7176e-05\n",
      "Epoch 117/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 3.5987e-05\n",
      "Epoch 118/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.4829e-05\n",
      "Epoch 119/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.3731e-05\n",
      "Epoch 120/1000\n",
      "48/48 [==============================] - 0s 754us/step - loss: 3.2657e-05\n",
      "Epoch 121/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.1615e-05\n",
      "Epoch 122/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 3.0615e-05\n",
      "Epoch 123/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.9659e-05\n",
      "Epoch 124/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2.8739e-05\n",
      "Epoch 125/1000\n",
      "48/48 [==============================] - 0s 729us/step - loss: 2.7846e-05\n",
      "Epoch 126/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.6983e-05\n",
      "Epoch 127/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.6145e-05\n",
      "Epoch 128/1000\n",
      "48/48 [==============================] - 0s 762us/step - loss: 2.5343e-05\n",
      "Epoch 129/1000\n",
      "48/48 [==============================] - 0s 786us/step - loss: 2.4567e-05\n",
      "Epoch 130/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 2.3818e-05\n",
      "Epoch 131/1000\n",
      "48/48 [==============================] - 0s 817us/step - loss: 2.3086e-05\n",
      "Epoch 132/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 2.2395e-05\n",
      "Epoch 133/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 2.1712e-05\n",
      "Epoch 134/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.1067e-05\n",
      "Epoch 135/1000\n",
      "48/48 [==============================] - 0s 737us/step - loss: 2.0431e-05\n",
      "Epoch 136/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.9814e-05\n",
      "Epoch 137/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.9218e-05\n",
      "Epoch 138/1000\n",
      "48/48 [==============================] - 0s 763us/step - loss: 1.8640e-05\n",
      "Epoch 139/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.8084e-05\n",
      "Epoch 140/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1.7550e-05\n",
      "Epoch 141/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 1.7029e-05\n",
      "Epoch 142/1000\n",
      "48/48 [==============================] - 0s 754us/step - loss: 1.6521e-05\n",
      "Epoch 143/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.6038e-05\n",
      "Epoch 144/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.5569e-05\n",
      "Epoch 145/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.5115e-05\n",
      "Epoch 146/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.4666e-05\n",
      "Epoch 147/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.4241e-05\n",
      "Epoch 148/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.3767e-05\n",
      "Epoch 149/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.3366e-05\n",
      "Epoch 150/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.2977e-05\n",
      "Epoch 151/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2604e-05\n",
      "Epoch 152/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.2247e-05\n",
      "Epoch 153/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.1896e-05\n",
      "Epoch 154/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1560e-05\n",
      "Epoch 155/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.1229e-05\n",
      "Epoch 156/1000\n",
      "48/48 [==============================] - 0s 794us/step - loss: 1.0909e-05\n",
      "Epoch 157/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 1.0601e-05\n",
      "Epoch 158/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.0303e-05\n",
      "Epoch 159/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.0009e-05\n",
      "Epoch 160/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 9.7240e-06\n",
      "Epoch 161/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 9.4499e-06\n",
      "Epoch 162/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.1841e-06\n",
      "Epoch 163/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 8.9236e-06\n",
      "Epoch 164/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 8.6748e-06\n",
      "Epoch 165/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 8.4325e-06\n",
      "Epoch 166/1000\n",
      "48/48 [==============================] - 0s 759us/step - loss: 8.1983e-06\n",
      "Epoch 167/1000\n",
      "48/48 [==============================] - 0s 795us/step - loss: 7.9706e-06\n",
      "Epoch 168/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 7.7485e-06\n",
      "Epoch 169/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.5353e-06\n",
      "Epoch 170/1000\n",
      "48/48 [==============================] - 0s 781us/step - loss: 7.3275e-06\n",
      "Epoch 171/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 7.1250e-06\n",
      "Epoch 172/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 6.9304e-06\n",
      "Epoch 173/1000\n",
      "48/48 [==============================] - 0s 819us/step - loss: 6.7399e-06\n",
      "Epoch 174/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.5551e-06\n",
      "Epoch 175/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 6.3759e-06\n",
      "Epoch 176/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.2035e-06\n",
      "Epoch 177/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.0345e-06\n",
      "Epoch 178/1000\n",
      "48/48 [==============================] - 0s 802us/step - loss: 5.8710e-06\n",
      "Epoch 179/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 5.7116e-06\n",
      "Epoch 180/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.5553e-06\n",
      "Epoch 181/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 5.4055e-06\n",
      "Epoch 182/1000\n",
      "48/48 [==============================] - 0s 800us/step - loss: 5.2586e-06\n",
      "Epoch 183/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 5.1174e-06\n",
      "Epoch 184/1000\n",
      "48/48 [==============================] - 0s 796us/step - loss: 4.9800e-06\n",
      "Epoch 185/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 4.8469e-06\n",
      "Epoch 186/1000\n",
      "48/48 [==============================] - 0s 806us/step - loss: 4.7157e-06\n",
      "Epoch 187/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 4.5899e-06\n",
      "Epoch 188/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 4.4655e-06\n",
      "Epoch 189/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 4.3464e-06\n",
      "Epoch 190/1000\n",
      "48/48 [==============================] - 0s 772us/step - loss: 4.2290e-06\n",
      "Epoch 191/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 4.1179e-06\n",
      "Epoch 192/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 4.0069e-06\n",
      "Epoch 193/1000\n",
      "48/48 [==============================] - 0s 847us/step - loss: 3.9020e-06\n",
      "Epoch 194/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 3.7967e-06\n",
      "Epoch 195/1000\n",
      "48/48 [==============================] - 0s 775us/step - loss: 3.6959e-06\n",
      "Epoch 196/1000\n",
      "48/48 [==============================] - 0s 794us/step - loss: 3.5986e-06\n",
      "Epoch 197/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 3.5031e-06\n",
      "Epoch 198/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 3.4115e-06\n",
      "Epoch 199/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 3.3218e-06\n",
      "Epoch 200/1000\n",
      "48/48 [==============================] - 0s 805us/step - loss: 3.2345e-06\n",
      "Epoch 201/1000\n",
      "48/48 [==============================] - 0s 828us/step - loss: 3.1499e-06\n",
      "Epoch 202/1000\n",
      "48/48 [==============================] - 0s 797us/step - loss: 3.0674e-06\n",
      "Epoch 203/1000\n",
      "48/48 [==============================] - 0s 819us/step - loss: 2.9865e-06\n",
      "Epoch 204/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.9089e-06\n",
      "Epoch 205/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.8329e-06\n",
      "Epoch 206/1000\n",
      "48/48 [==============================] - 0s 819us/step - loss: 2.7594e-06\n",
      "Epoch 207/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 2.6879e-06\n",
      "Epoch 208/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 2.6185e-06\n",
      "Epoch 209/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.5514e-06\n",
      "Epoch 210/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.4854e-06\n",
      "Epoch 211/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.4211e-06\n",
      "Epoch 212/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2.3588e-06\n",
      "Epoch 213/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.2982e-06\n",
      "Epoch 214/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.2393e-06\n",
      "Epoch 215/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.1824e-06\n",
      "Epoch 216/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2.1258e-06\n",
      "Epoch 217/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 2.0718e-06\n",
      "Epoch 218/1000\n",
      "48/48 [==============================] - 0s 832us/step - loss: 2.0191e-06\n",
      "Epoch 219/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.9681e-06\n",
      "Epoch 220/1000\n",
      "48/48 [==============================] - 0s 775us/step - loss: 1.9183e-06\n",
      "Epoch 221/1000\n",
      "48/48 [==============================] - 0s 785us/step - loss: 1.8700e-06\n",
      "Epoch 222/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.8220e-06\n",
      "Epoch 223/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 1.7768e-06\n",
      "Epoch 224/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.7319e-06\n",
      "Epoch 225/1000\n",
      "48/48 [==============================] - 0s 783us/step - loss: 1.6883e-06\n",
      "Epoch 226/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.6414e-06\n",
      "Epoch 227/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.5986e-06\n",
      "Epoch 228/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.5588e-06\n",
      "Epoch 229/1000\n",
      "48/48 [==============================] - 0s 801us/step - loss: 1.5202e-06\n",
      "Epoch 230/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.4829e-06\n",
      "Epoch 231/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.4459e-06\n",
      "Epoch 232/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.4107e-06\n",
      "Epoch 233/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 1.3758e-06\n",
      "Epoch 234/1000\n",
      "48/48 [==============================] - 0s 936us/step - loss: 1.3418e-06\n",
      "Epoch 235/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 1.3092e-06\n",
      "Epoch 236/1000\n",
      "48/48 [==============================] - 0s 829us/step - loss: 1.2767e-06\n",
      "Epoch 237/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2458e-06\n",
      "Epoch 238/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2158e-06\n",
      "Epoch 239/1000\n",
      "48/48 [==============================] - 0s 855us/step - loss: 1.1862e-06\n",
      "Epoch 240/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1576e-06\n",
      "Epoch 241/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.1295e-06\n",
      "Epoch 242/1000\n",
      "48/48 [==============================] - 0s 857us/step - loss: 1.1020e-06\n",
      "Epoch 243/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.0757e-06\n",
      "Epoch 244/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 1.0499e-06\n",
      "Epoch 245/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 1.0249e-06\n",
      "Epoch 246/1000\n",
      "48/48 [==============================] - 0s 834us/step - loss: 1.0002e-06\n",
      "Epoch 247/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 9.7640e-07\n",
      "Epoch 248/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 9.5311e-07\n",
      "Epoch 249/1000\n",
      "48/48 [==============================] - 0s 817us/step - loss: 9.3041e-07\n",
      "Epoch 250/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 9.0852e-07\n",
      "Epoch 251/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 8.8528e-07\n",
      "Epoch 252/1000\n",
      "48/48 [==============================] - 0s 801us/step - loss: 8.6350e-07\n",
      "Epoch 253/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 8.4331e-07\n",
      "Epoch 254/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 8.2352e-07\n",
      "Epoch 255/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 8.0248e-07\n",
      "Epoch 256/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 7.8355e-07\n",
      "Epoch 257/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 7.6555e-07\n",
      "Epoch 258/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 7.4802e-07\n",
      "Epoch 259/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 7.3080e-07\n",
      "Epoch 260/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.1407e-07\n",
      "Epoch 261/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 6.9784e-07\n",
      "Epoch 262/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 6.8205e-07\n",
      "Epoch 263/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 6.6647e-07\n",
      "Epoch 264/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 6.5148e-07\n",
      "Epoch 265/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 6.3689e-07\n",
      "Epoch 266/1000\n",
      "48/48 [==============================] - 0s 755us/step - loss: 6.2231e-07\n",
      "Epoch 267/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 6.0847e-07\n",
      "Epoch 268/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.9491e-07\n",
      "Epoch 269/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.8150e-07\n",
      "Epoch 270/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 5.6872e-07\n",
      "Epoch 271/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 5.5617e-07\n",
      "Epoch 272/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 5.4396e-07\n",
      "Epoch 273/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.3198e-07\n",
      "Epoch 274/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 5.2044e-07\n",
      "Epoch 275/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 5.0892e-07\n",
      "Epoch 276/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 4.9794e-07\n",
      "Epoch 277/1000\n",
      "48/48 [==============================] - 0s 727us/step - loss: 4.8719e-07\n",
      "Epoch 278/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 4.7670e-07\n",
      "Epoch 279/1000\n",
      "48/48 [==============================] - 0s 819us/step - loss: 4.6641e-07\n",
      "Epoch 280/1000\n",
      "48/48 [==============================] - 0s 979us/step - loss: 4.5645e-07\n",
      "Epoch 281/1000\n",
      "48/48 [==============================] - 0s 831us/step - loss: 4.4686e-07\n",
      "Epoch 282/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 4.3724e-07\n",
      "Epoch 283/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 4.2802e-07\n",
      "Epoch 284/1000\n",
      "48/48 [==============================] - 0s 796us/step - loss: 4.1903e-07\n",
      "Epoch 285/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 4.1031e-07\n",
      "Epoch 286/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 4.0176e-07\n",
      "Epoch 287/1000\n",
      "48/48 [==============================] - 0s 979us/step - loss: 3.9333e-07\n",
      "Epoch 288/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 3.8513e-07\n",
      "Epoch 289/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 3.7718e-07\n",
      "Epoch 290/1000\n",
      "48/48 [==============================] - 0s 782us/step - loss: 3.6945e-07\n",
      "Epoch 291/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 3.6193e-07\n",
      "Epoch 292/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 3.5456e-07\n",
      "Epoch 293/1000\n",
      "48/48 [==============================] - 0s 938us/step - loss: 3.4735e-07\n",
      "Epoch 294/1000\n",
      "48/48 [==============================] - 0s 796us/step - loss: 3.4030e-07\n",
      "Epoch 295/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 3.3352e-07\n",
      "Epoch 296/1000\n",
      "48/48 [==============================] - 0s 774us/step - loss: 3.2687e-07\n",
      "Epoch 297/1000\n",
      "48/48 [==============================] - 0s 796us/step - loss: 3.2044e-07\n",
      "Epoch 298/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 3.1408e-07\n",
      "Epoch 299/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 3.0795e-07\n",
      "Epoch 300/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 3.0192e-07\n",
      "Epoch 301/1000\n",
      "48/48 [==============================] - 0s 811us/step - loss: 2.9602e-07\n",
      "Epoch 302/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.9030e-07\n",
      "Epoch 303/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 2.8468e-07\n",
      "Epoch 304/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 2.7923e-07\n",
      "Epoch 305/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.7387e-07\n",
      "Epoch 306/1000\n",
      "48/48 [==============================] - 0s 747us/step - loss: 2.6872e-07\n",
      "Epoch 307/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.6360e-07\n",
      "Epoch 308/1000\n",
      "48/48 [==============================] - 0s 815us/step - loss: 2.5870e-07\n",
      "Epoch 309/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.5383e-07\n",
      "Epoch 310/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.4913e-07\n",
      "Epoch 311/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 2.4452e-07\n",
      "Epoch 312/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2.4009e-07\n",
      "Epoch 313/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.3566e-07\n",
      "Epoch 314/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.3132e-07\n",
      "Epoch 315/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.2713e-07\n",
      "Epoch 316/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.2301e-07\n",
      "Epoch 317/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.1900e-07\n",
      "Epoch 318/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 2.1513e-07\n",
      "Epoch 319/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.1130e-07\n",
      "Epoch 320/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.0757e-07\n",
      "Epoch 321/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 2.0396e-07\n",
      "Epoch 322/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.0041e-07\n",
      "Epoch 323/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 1.9697e-07\n",
      "Epoch 324/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.9357e-07\n",
      "Epoch 325/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.8997e-07\n",
      "Epoch 326/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.8671e-07\n",
      "Epoch 327/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.8354e-07\n",
      "Epoch 328/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.8049e-07\n",
      "Epoch 329/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 1.7754e-07\n",
      "Epoch 330/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.7460e-07\n",
      "Epoch 331/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7172e-07\n",
      "Epoch 332/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.6893e-07\n",
      "Epoch 333/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.6619e-07\n",
      "Epoch 334/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 1.6354e-07\n",
      "Epoch 335/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.6093e-07\n",
      "Epoch 336/1000\n",
      "48/48 [==============================] - 0s 703us/step - loss: 1.5837e-07\n",
      "Epoch 337/1000\n",
      "48/48 [==============================] - 0s 712us/step - loss: 1.5584e-07\n",
      "Epoch 338/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.5317e-07\n",
      "Epoch 339/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1.5075e-07\n",
      "Epoch 340/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.4839e-07\n",
      "Epoch 341/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.4612e-07\n",
      "Epoch 342/1000\n",
      "48/48 [==============================] - 0s 761us/step - loss: 1.4390e-07\n",
      "Epoch 343/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.4166e-07\n",
      "Epoch 344/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3951e-07\n",
      "Epoch 345/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.3743e-07\n",
      "Epoch 346/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.3539e-07\n",
      "Epoch 347/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.3341e-07\n",
      "Epoch 348/1000\n",
      "48/48 [==============================] - 0s 729us/step - loss: 1.3146e-07\n",
      "Epoch 349/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 1.2958e-07\n",
      "Epoch 350/1000\n",
      "48/48 [==============================] - 0s 750us/step - loss: 1.2769e-07\n",
      "Epoch 351/1000\n",
      "48/48 [==============================] - 0s 747us/step - loss: 1.2584e-07\n",
      "Epoch 352/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1.2405e-07\n",
      "Epoch 353/1000\n",
      "48/48 [==============================] - 0s 758us/step - loss: 1.2230e-07\n",
      "Epoch 354/1000\n",
      "48/48 [==============================] - 0s 760us/step - loss: 1.2058e-07\n",
      "Epoch 355/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1891e-07\n",
      "Epoch 356/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.1725e-07\n",
      "Epoch 357/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.1564e-07\n",
      "Epoch 358/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 1.1406e-07\n",
      "Epoch 359/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1252e-07\n",
      "Epoch 360/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1101e-07\n",
      "Epoch 361/1000\n",
      "48/48 [==============================] - 0s 730us/step - loss: 1.0951e-07\n",
      "Epoch 362/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.0806e-07\n",
      "Epoch 363/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0662e-07\n",
      "Epoch 364/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.0524e-07\n",
      "Epoch 365/1000\n",
      "48/48 [==============================] - 0s 920us/step - loss: 1.0387e-07\n",
      "Epoch 366/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.0253e-07\n",
      "Epoch 367/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0123e-07\n",
      "Epoch 368/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 9.9928e-08\n",
      "Epoch 369/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.8667e-08\n",
      "Epoch 370/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 9.7432e-08\n",
      "Epoch 371/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 9.6218e-08\n",
      "Epoch 372/1000\n",
      "48/48 [==============================] - 0s 794us/step - loss: 9.5028e-08\n",
      "Epoch 373/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 9.3867e-08\n",
      "Epoch 374/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.2716e-08\n",
      "Epoch 375/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.1590e-08\n",
      "Epoch 376/1000\n",
      "48/48 [==============================] - 0s 774us/step - loss: 9.0487e-08\n",
      "Epoch 377/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.9318e-08\n",
      "Epoch 378/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 8.8249e-08\n",
      "Epoch 379/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.7210e-08\n",
      "Epoch 380/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.6188e-08\n",
      "Epoch 381/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 8.5194e-08\n",
      "Epoch 382/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 8.4218e-08\n",
      "Epoch 383/1000\n",
      "48/48 [==============================] - 0s 789us/step - loss: 8.3258e-08\n",
      "Epoch 384/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 8.2317e-08\n",
      "Epoch 385/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.1397e-08\n",
      "Epoch 386/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 8.0483e-08\n",
      "Epoch 387/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.9593e-08\n",
      "Epoch 388/1000\n",
      "48/48 [==============================] - 0s 779us/step - loss: 7.8707e-08\n",
      "Epoch 389/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.7850e-08\n",
      "Epoch 390/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.7000e-08\n",
      "Epoch 391/1000\n",
      "48/48 [==============================] - 0s 749us/step - loss: 7.6172e-08\n",
      "Epoch 392/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 7.5349e-08\n",
      "Epoch 393/1000\n",
      "48/48 [==============================] - 0s 769us/step - loss: 7.4548e-08\n",
      "Epoch 394/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 7.3753e-08\n",
      "Epoch 395/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 7.2981e-08\n",
      "Epoch 396/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 7.2225e-08\n",
      "Epoch 397/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 7.1476e-08\n",
      "Epoch 398/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 7.0746e-08\n",
      "Epoch 399/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 7.0027e-08\n",
      "Epoch 400/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 6.9320e-08\n",
      "Epoch 401/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 6.8624e-08\n",
      "Epoch 402/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 6.7941e-08\n",
      "Epoch 403/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 6.7265e-08\n",
      "Epoch 404/1000\n",
      "48/48 [==============================] - 0s 840us/step - loss: 6.6602e-08\n",
      "Epoch 405/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 6.5952e-08\n",
      "Epoch 406/1000\n",
      "48/48 [==============================] - 0s 755us/step - loss: 6.5311e-08\n",
      "Epoch 407/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 6.4683e-08\n",
      "Epoch 408/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.4064e-08\n",
      "Epoch 409/1000\n",
      "48/48 [==============================] - 0s 733us/step - loss: 6.3458e-08\n",
      "Epoch 410/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 6.2856e-08\n",
      "Epoch 411/1000\n",
      "48/48 [==============================] - 0s 969us/step - loss: 6.2267e-08\n",
      "Epoch 412/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 6.1689e-08\n",
      "Epoch 413/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 6.1118e-08\n",
      "Epoch 414/1000\n",
      "48/48 [==============================] - 0s 772us/step - loss: 6.0556e-08\n",
      "Epoch 415/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 6.0004e-08\n",
      "Epoch 416/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 5.9460e-08\n",
      "Epoch 417/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.8925e-08\n",
      "Epoch 418/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 5.8399e-08\n",
      "Epoch 419/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 5.7882e-08\n",
      "Epoch 420/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 5.7361e-08\n",
      "Epoch 421/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 5.6860e-08\n",
      "Epoch 422/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 5.6362e-08\n",
      "Epoch 423/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.5872e-08\n",
      "Epoch 424/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 5.5392e-08\n",
      "Epoch 425/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.4922e-08\n",
      "Epoch 426/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 5.4455e-08\n",
      "Epoch 427/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 5.3994e-08\n",
      "Epoch 428/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 5.3543e-08\n",
      "Epoch 429/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 5.3099e-08\n",
      "Epoch 430/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 5.2633e-08\n",
      "Epoch 431/1000\n",
      "48/48 [==============================] - 0s 752us/step - loss: 5.2189e-08\n",
      "Epoch 432/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 5.1762e-08\n",
      "Epoch 433/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 5.1343e-08\n",
      "Epoch 434/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 5.0931e-08\n",
      "Epoch 435/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 5.0519e-08\n",
      "Epoch 436/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 5.0119e-08\n",
      "Epoch 437/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 4.9725e-08\n",
      "Epoch 438/1000\n",
      "48/48 [==============================] - 0s 752us/step - loss: 4.9337e-08\n",
      "Epoch 439/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 4.8952e-08\n",
      "Epoch 440/1000\n",
      "48/48 [==============================] - 0s 725us/step - loss: 4.8574e-08\n",
      "Epoch 441/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 4.8201e-08\n",
      "Epoch 442/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 4.7833e-08\n",
      "Epoch 443/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 4.7470e-08\n",
      "Epoch 444/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 4.7107e-08\n",
      "Epoch 445/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 4.6750e-08\n",
      "Epoch 446/1000\n",
      "48/48 [==============================] - 0s 751us/step - loss: 4.6404e-08\n",
      "Epoch 447/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 4.6060e-08\n",
      "Epoch 448/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 4.5718e-08\n",
      "Epoch 449/1000\n",
      "48/48 [==============================] - 0s 769us/step - loss: 4.5386e-08\n",
      "Epoch 450/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 4.5051e-08\n",
      "Epoch 451/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 4.4708e-08\n",
      "Epoch 452/1000\n",
      "48/48 [==============================] - 0s 755us/step - loss: 4.4377e-08\n",
      "Epoch 453/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 4.4058e-08\n",
      "Epoch 454/1000\n",
      "48/48 [==============================] - 0s 779us/step - loss: 4.3746e-08\n",
      "Epoch 455/1000\n",
      "48/48 [==============================] - 0s 1000us/step - loss: 4.3438e-08\n",
      "Epoch 456/1000\n",
      "48/48 [==============================] - 0s 886us/step - loss: 4.3129e-08\n",
      "Epoch 457/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 4.2829e-08\n",
      "Epoch 458/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 4.2530e-08\n",
      "Epoch 459/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 4.2238e-08\n",
      "Epoch 460/1000\n",
      "48/48 [==============================] - 0s 744us/step - loss: 4.1947e-08\n",
      "Epoch 461/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 4.1661e-08\n",
      "Epoch 462/1000\n",
      "48/48 [==============================] - 0s 739us/step - loss: 4.1378e-08\n",
      "Epoch 463/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 4.1100e-08\n",
      "Epoch 464/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 4.0823e-08\n",
      "Epoch 465/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 4.0553e-08\n",
      "Epoch 466/1000\n",
      "48/48 [==============================] - 0s 747us/step - loss: 4.0285e-08\n",
      "Epoch 467/1000\n",
      "48/48 [==============================] - 0s 741us/step - loss: 4.0019e-08\n",
      "Epoch 468/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.9760e-08\n",
      "Epoch 469/1000\n",
      "48/48 [==============================] - 0s 870us/step - loss: 3.9499e-08\n",
      "Epoch 470/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 3.9243e-08\n",
      "Epoch 471/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.8990e-08\n",
      "Epoch 472/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 3.8741e-08\n",
      "Epoch 473/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.8494e-08\n",
      "Epoch 474/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 3.8248e-08\n",
      "Epoch 475/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.8006e-08\n",
      "Epoch 476/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.7753e-08\n",
      "Epoch 477/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 3.7515e-08\n",
      "Epoch 478/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.7282e-08\n",
      "Epoch 479/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 3.7052e-08\n",
      "Epoch 480/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 3.6825e-08\n",
      "Epoch 481/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 3.6601e-08\n",
      "Epoch 482/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 3.6379e-08\n",
      "Epoch 483/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.6160e-08\n",
      "Epoch 484/1000\n",
      "48/48 [==============================] - 0s 957us/step - loss: 3.5943e-08\n",
      "Epoch 485/1000\n",
      "48/48 [==============================] - 0s 854us/step - loss: 3.5731e-08\n",
      "Epoch 486/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 3.5517e-08\n",
      "Epoch 487/1000\n",
      "48/48 [==============================] - 0s 721us/step - loss: 3.5308e-08\n",
      "Epoch 488/1000\n",
      "48/48 [==============================] - 0s 743us/step - loss: 3.5100e-08\n",
      "Epoch 489/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 3.4896e-08\n",
      "Epoch 490/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 3.4694e-08\n",
      "Epoch 491/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.4494e-08\n",
      "Epoch 492/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 3.4295e-08\n",
      "Epoch 493/1000\n",
      "48/48 [==============================] - 0s 750us/step - loss: 3.4101e-08\n",
      "Epoch 494/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.3907e-08\n",
      "Epoch 495/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 3.3714e-08\n",
      "Epoch 496/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.3525e-08\n",
      "Epoch 497/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 3.3337e-08\n",
      "Epoch 498/1000\n",
      "48/48 [==============================] - 0s 882us/step - loss: 3.3152e-08\n",
      "Epoch 499/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.2968e-08\n",
      "Epoch 500/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 3.2786e-08\n",
      "Epoch 501/1000\n",
      "48/48 [==============================] - 0s 772us/step - loss: 3.2607e-08\n",
      "Epoch 502/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 3.2427e-08\n",
      "Epoch 503/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 3.2239e-08\n",
      "Epoch 504/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 3.2063e-08\n",
      "Epoch 505/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 3.1890e-08\n",
      "Epoch 506/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 3.1706e-08\n",
      "Epoch 507/1000\n",
      "48/48 [==============================] - 0s 760us/step - loss: 3.1536e-08\n",
      "Epoch 508/1000\n",
      "48/48 [==============================] - 0s 775us/step - loss: 3.1369e-08\n",
      "Epoch 509/1000\n",
      "48/48 [==============================] - 0s 915us/step - loss: 3.1203e-08\n",
      "Epoch 510/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 3.1039e-08\n",
      "Epoch 511/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 3.0876e-08\n",
      "Epoch 512/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3.0716e-08\n",
      "Epoch 513/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 3.0556e-08\n",
      "Epoch 514/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 3.0400e-08\n",
      "Epoch 515/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 3.0243e-08\n",
      "Epoch 516/1000\n",
      "48/48 [==============================] - 0s 781us/step - loss: 3.0088e-08\n",
      "Epoch 517/1000\n",
      "48/48 [==============================] - 0s 718us/step - loss: 2.9937e-08\n",
      "Epoch 518/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.9786e-08\n",
      "Epoch 519/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.9636e-08\n",
      "Epoch 520/1000\n",
      "48/48 [==============================] - 0s 702us/step - loss: 2.9485e-08\n",
      "Epoch 521/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 2.9340e-08\n",
      "Epoch 522/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2.9193e-08\n",
      "Epoch 523/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.9049e-08\n",
      "Epoch 524/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.8906e-08\n",
      "Epoch 525/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2.8763e-08\n",
      "Epoch 526/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.8624e-08\n",
      "Epoch 527/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.8484e-08\n",
      "Epoch 528/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 2.8346e-08\n",
      "Epoch 529/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.8209e-08\n",
      "Epoch 530/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.8075e-08\n",
      "Epoch 531/1000\n",
      "48/48 [==============================] - 0s 755us/step - loss: 2.7941e-08\n",
      "Epoch 532/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.7807e-08\n",
      "Epoch 533/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2.7675e-08\n",
      "Epoch 534/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 2.7545e-08\n",
      "Epoch 535/1000\n",
      "48/48 [==============================] - 0s 957us/step - loss: 2.7416e-08\n",
      "Epoch 536/1000\n",
      "48/48 [==============================] - 0s 853us/step - loss: 2.7287e-08\n",
      "Epoch 537/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.7161e-08\n",
      "Epoch 538/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.7035e-08\n",
      "Epoch 539/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 2.6912e-08\n",
      "Epoch 540/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.6787e-08\n",
      "Epoch 541/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.6665e-08\n",
      "Epoch 542/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.6544e-08\n",
      "Epoch 543/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 2.6422e-08\n",
      "Epoch 544/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.6301e-08\n",
      "Epoch 545/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.6184e-08\n",
      "Epoch 546/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.6066e-08\n",
      "Epoch 547/1000\n",
      "48/48 [==============================] - 0s 771us/step - loss: 2.5950e-08\n",
      "Epoch 548/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.5834e-08\n",
      "Epoch 549/1000\n",
      "48/48 [==============================] - 0s 862us/step - loss: 2.5720e-08\n",
      "Epoch 550/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.5607e-08\n",
      "Epoch 551/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.5493e-08\n",
      "Epoch 552/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.5382e-08\n",
      "Epoch 553/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.5272e-08\n",
      "Epoch 554/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 2.5163e-08\n",
      "Epoch 555/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.5054e-08\n",
      "Epoch 556/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 2.4947e-08\n",
      "Epoch 557/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.4840e-08\n",
      "Epoch 558/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.4734e-08\n",
      "Epoch 559/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 2.4629e-08\n",
      "Epoch 560/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.4525e-08\n",
      "Epoch 561/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.4415e-08\n",
      "Epoch 562/1000\n",
      "48/48 [==============================] - 0s 857us/step - loss: 2.4312e-08\n",
      "Epoch 563/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 2.4210e-08\n",
      "Epoch 564/1000\n",
      "48/48 [==============================] - 0s 812us/step - loss: 2.4109e-08\n",
      "Epoch 565/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.4009e-08\n",
      "Epoch 566/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 2.3910e-08\n",
      "Epoch 567/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.3811e-08\n",
      "Epoch 568/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.3713e-08\n",
      "Epoch 569/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.3616e-08\n",
      "Epoch 570/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.3520e-08\n",
      "Epoch 571/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.3425e-08\n",
      "Epoch 572/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 2.3330e-08\n",
      "Epoch 573/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.3236e-08\n",
      "Epoch 574/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.3142e-08\n",
      "Epoch 575/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 2.3049e-08\n",
      "Epoch 576/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.2958e-08\n",
      "Epoch 577/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 2.2867e-08\n",
      "Epoch 578/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.2777e-08\n",
      "Epoch 579/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 2.2687e-08\n",
      "Epoch 580/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.2598e-08\n",
      "Epoch 581/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.2509e-08\n",
      "Epoch 582/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.2422e-08\n",
      "Epoch 583/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.2334e-08\n",
      "Epoch 584/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 2.2249e-08\n",
      "Epoch 585/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2.2163e-08\n",
      "Epoch 586/1000\n",
      "48/48 [==============================] - 0s 852us/step - loss: 2.2078e-08\n",
      "Epoch 587/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 2.1993e-08\n",
      "Epoch 588/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2.1910e-08\n",
      "Epoch 589/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 2.1826e-08\n",
      "Epoch 590/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.1743e-08\n",
      "Epoch 591/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 2.1662e-08\n",
      "Epoch 592/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.1580e-08\n",
      "Epoch 593/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.1500e-08\n",
      "Epoch 594/1000\n",
      "48/48 [==============================] - 0s 755us/step - loss: 2.1420e-08\n",
      "Epoch 595/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.1340e-08\n",
      "Epoch 596/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.1261e-08\n",
      "Epoch 597/1000\n",
      "48/48 [==============================] - 0s 883us/step - loss: 2.1183e-08\n",
      "Epoch 598/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 2.1105e-08\n",
      "Epoch 599/1000\n",
      "48/48 [==============================] - 0s 818us/step - loss: 2.1028e-08\n",
      "Epoch 600/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2.0951e-08\n",
      "Epoch 601/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 2.0874e-08\n",
      "Epoch 602/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.0798e-08\n",
      "Epoch 603/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.0723e-08\n",
      "Epoch 604/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.0648e-08\n",
      "Epoch 605/1000\n",
      "48/48 [==============================] - 0s 761us/step - loss: 2.0574e-08\n",
      "Epoch 606/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2.0500e-08\n",
      "Epoch 607/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2.0427e-08\n",
      "Epoch 608/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2.0354e-08\n",
      "Epoch 609/1000\n",
      "48/48 [==============================] - 0s 905us/step - loss: 2.0282e-08\n",
      "Epoch 610/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 2.0211e-08\n",
      "Epoch 611/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 2.0139e-08\n",
      "Epoch 612/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 2.0069e-08\n",
      "Epoch 613/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.9999e-08\n",
      "Epoch 614/1000\n",
      "48/48 [==============================] - 0s 753us/step - loss: 1.9929e-08\n",
      "Epoch 615/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.9860e-08\n",
      "Epoch 616/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.9791e-08\n",
      "Epoch 617/1000\n",
      "48/48 [==============================] - 0s 753us/step - loss: 1.9722e-08\n",
      "Epoch 618/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.9655e-08\n",
      "Epoch 619/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1.9587e-08\n",
      "Epoch 620/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 1.9521e-08\n",
      "Epoch 621/1000\n",
      "48/48 [==============================] - 0s 957us/step - loss: 1.9454e-08\n",
      "Epoch 622/1000\n",
      "48/48 [==============================] - 0s 859us/step - loss: 1.9388e-08\n",
      "Epoch 623/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.9323e-08\n",
      "Epoch 624/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.9258e-08\n",
      "Epoch 625/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.9193e-08\n",
      "Epoch 626/1000\n",
      "48/48 [==============================] - 0s 772us/step - loss: 1.9129e-08\n",
      "Epoch 627/1000\n",
      "48/48 [==============================] - 0s 751us/step - loss: 1.9065e-08\n",
      "Epoch 628/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.9002e-08\n",
      "Epoch 629/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.8938e-08\n",
      "Epoch 630/1000\n",
      "48/48 [==============================] - 0s 750us/step - loss: 1.8876e-08\n",
      "Epoch 631/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.8813e-08\n",
      "Epoch 632/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.8752e-08\n",
      "Epoch 633/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.8690e-08\n",
      "Epoch 634/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 1.8629e-08\n",
      "Epoch 635/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.8568e-08\n",
      "Epoch 636/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.8508e-08\n",
      "Epoch 637/1000\n",
      "48/48 [==============================] - 0s 769us/step - loss: 1.8448e-08\n",
      "Epoch 638/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.8388e-08\n",
      "Epoch 639/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.8328e-08\n",
      "Epoch 640/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 1.8269e-08\n",
      "Epoch 641/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.8211e-08\n",
      "Epoch 642/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 1.8153e-08\n",
      "Epoch 643/1000\n",
      "48/48 [==============================] - 0s 957us/step - loss: 1.8095e-08\n",
      "Epoch 644/1000\n",
      "48/48 [==============================] - 0s 863us/step - loss: 1.8037e-08\n",
      "Epoch 645/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.7980e-08\n",
      "Epoch 646/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7924e-08\n",
      "Epoch 647/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1.7867e-08\n",
      "Epoch 648/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.7811e-08\n",
      "Epoch 649/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.7755e-08\n",
      "Epoch 650/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.7700e-08\n",
      "Epoch 651/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7645e-08\n",
      "Epoch 652/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 1.7590e-08\n",
      "Epoch 653/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7536e-08\n",
      "Epoch 654/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.7482e-08\n",
      "Epoch 655/1000\n",
      "48/48 [==============================] - 0s 990us/step - loss: 1.7428e-08\n",
      "Epoch 656/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.7375e-08\n",
      "Epoch 657/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.7322e-08\n",
      "Epoch 658/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.7269e-08\n",
      "Epoch 659/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7216e-08\n",
      "Epoch 660/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.7164e-08\n",
      "Epoch 661/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7112e-08\n",
      "Epoch 662/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.7060e-08\n",
      "Epoch 663/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.7009e-08\n",
      "Epoch 664/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.6958e-08\n",
      "Epoch 665/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 1.6907e-08\n",
      "Epoch 666/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.6857e-08\n",
      "Epoch 667/1000\n",
      "48/48 [==============================] - 0s 915us/step - loss: 1.6807e-08\n",
      "Epoch 668/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.6757e-08\n",
      "Epoch 669/1000\n",
      "48/48 [==============================] - 0s 883us/step - loss: 1.6708e-08\n",
      "Epoch 670/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.6659e-08\n",
      "Epoch 671/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 1.6610e-08\n",
      "Epoch 672/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.6561e-08\n",
      "Epoch 673/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 1.6513e-08\n",
      "Epoch 674/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.6465e-08\n",
      "Epoch 675/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.6417e-08\n",
      "Epoch 676/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.6370e-08\n",
      "Epoch 677/1000\n",
      "48/48 [==============================] - 0s 979us/step - loss: 1.6323e-08\n",
      "Epoch 678/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 1.6275e-08\n",
      "Epoch 679/1000\n",
      "48/48 [==============================] - 0s 915us/step - loss: 1.6229e-08\n",
      "Epoch 680/1000\n",
      "48/48 [==============================] - 0s 811us/step - loss: 1.6182e-08\n",
      "Epoch 681/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.6136e-08\n",
      "Epoch 682/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.6090e-08\n",
      "Epoch 683/1000\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1.6044e-08\n",
      "Epoch 684/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.5999e-08\n",
      "Epoch 685/1000\n",
      "48/48 [==============================] - 0s 863us/step - loss: 1.5953e-08\n",
      "Epoch 686/1000\n",
      "48/48 [==============================] - 0s 915us/step - loss: 1.5908e-08\n",
      "Epoch 687/1000\n",
      "48/48 [==============================] - 0s 884us/step - loss: 1.5864e-08\n",
      "Epoch 688/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.5819e-08\n",
      "Epoch 689/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.5775e-08\n",
      "Epoch 690/1000\n",
      "48/48 [==============================] - 0s 812us/step - loss: 1.5731e-08\n",
      "Epoch 691/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 1.5687e-08\n",
      "Epoch 692/1000\n",
      "48/48 [==============================] - 0s 778us/step - loss: 1.5644e-08\n",
      "Epoch 693/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.5599e-08\n",
      "Epoch 694/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.5556e-08\n",
      "Epoch 695/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.5513e-08\n",
      "Epoch 696/1000\n",
      "48/48 [==============================] - 0s 936us/step - loss: 1.5471e-08\n",
      "Epoch 697/1000\n",
      "48/48 [==============================] - 0s 905us/step - loss: 1.5426e-08\n",
      "Epoch 698/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.5383e-08\n",
      "Epoch 699/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.5341e-08\n",
      "Epoch 700/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.5300e-08\n",
      "Epoch 701/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.5258e-08\n",
      "Epoch 702/1000\n",
      "48/48 [==============================] - 0s 750us/step - loss: 1.5217e-08\n",
      "Epoch 703/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.5174e-08\n",
      "Epoch 704/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.5132e-08\n",
      "Epoch 705/1000\n",
      "48/48 [==============================] - 0s 749us/step - loss: 1.5089e-08\n",
      "Epoch 706/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.5048e-08\n",
      "Epoch 707/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.5008e-08\n",
      "Epoch 708/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.4968e-08\n",
      "Epoch 709/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 1.4928e-08\n",
      "Epoch 710/1000\n",
      "48/48 [==============================] - 0s 753us/step - loss: 1.4889e-08\n",
      "Epoch 711/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.4849e-08\n",
      "Epoch 712/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.4810e-08\n",
      "Epoch 713/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.4771e-08\n",
      "Epoch 714/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.4733e-08\n",
      "Epoch 715/1000\n",
      "48/48 [==============================] - 0s 726us/step - loss: 1.4694e-08\n",
      "Epoch 716/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 1.4656e-08\n",
      "Epoch 717/1000\n",
      "48/48 [==============================] - 0s 899us/step - loss: 1.4617e-08\n",
      "Epoch 718/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.4579e-08\n",
      "Epoch 719/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.4541e-08\n",
      "Epoch 720/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 1.4504e-08\n",
      "Epoch 721/1000\n",
      "48/48 [==============================] - 0s 808us/step - loss: 1.4467e-08\n",
      "Epoch 722/1000\n",
      "48/48 [==============================] - 0s 754us/step - loss: 1.4430e-08\n",
      "Epoch 723/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.4393e-08\n",
      "Epoch 724/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.4356e-08\n",
      "Epoch 725/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.4319e-08\n",
      "Epoch 726/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.4282e-08\n",
      "Epoch 727/1000\n",
      "48/48 [==============================] - 0s 969us/step - loss: 1.4246e-08\n",
      "Epoch 728/1000\n",
      "48/48 [==============================] - 0s 873us/step - loss: 1.4209e-08\n",
      "Epoch 729/1000\n",
      "48/48 [==============================] - 0s 863us/step - loss: 1.4173e-08\n",
      "Epoch 730/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.4138e-08\n",
      "Epoch 731/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.4102e-08\n",
      "Epoch 732/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.4066e-08\n",
      "Epoch 733/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.4031e-08\n",
      "Epoch 734/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.3996e-08\n",
      "Epoch 735/1000\n",
      "48/48 [==============================] - 0s 744us/step - loss: 1.3961e-08\n",
      "Epoch 736/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.3926e-08\n",
      "Epoch 737/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.3892e-08\n",
      "Epoch 738/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.3857e-08\n",
      "Epoch 739/1000\n",
      "48/48 [==============================] - 0s 791us/step - loss: 1.3823e-08\n",
      "Epoch 740/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.3789e-08\n",
      "Epoch 741/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.3755e-08\n",
      "Epoch 742/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 1.3721e-08\n",
      "Epoch 743/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.3688e-08\n",
      "Epoch 744/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.3654e-08\n",
      "Epoch 745/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.3621e-08\n",
      "Epoch 746/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.3588e-08\n",
      "Epoch 747/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.3555e-08\n",
      "Epoch 748/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.3522e-08\n",
      "Epoch 749/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 1.3489e-08\n",
      "Epoch 750/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.3457e-08\n",
      "Epoch 751/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.3424e-08\n",
      "Epoch 752/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.3392e-08\n",
      "Epoch 753/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.3360e-08\n",
      "Epoch 754/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.3328e-08\n",
      "Epoch 755/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3296e-08\n",
      "Epoch 756/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.3264e-08\n",
      "Epoch 757/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 1.3231e-08\n",
      "Epoch 758/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.3200e-08\n",
      "Epoch 759/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3168e-08\n",
      "Epoch 760/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3137e-08\n",
      "Epoch 761/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.3107e-08\n",
      "Epoch 762/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3076e-08\n",
      "Epoch 763/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.3045e-08\n",
      "Epoch 764/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1.3013e-08\n",
      "Epoch 765/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2983e-08\n",
      "Epoch 766/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 1.2953e-08\n",
      "Epoch 767/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.2923e-08\n",
      "Epoch 768/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.2893e-08\n",
      "Epoch 769/1000\n",
      "48/48 [==============================] - 0s 792us/step - loss: 1.2863e-08\n",
      "Epoch 770/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2834e-08\n",
      "Epoch 771/1000\n",
      "48/48 [==============================] - 0s 794us/step - loss: 1.2804e-08\n",
      "Epoch 772/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 1.2775e-08\n",
      "Epoch 773/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.2745e-08\n",
      "Epoch 774/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.2716e-08\n",
      "Epoch 775/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2687e-08\n",
      "Epoch 776/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.2659e-08\n",
      "Epoch 777/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.2630e-08\n",
      "Epoch 778/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 1.2601e-08\n",
      "Epoch 779/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2573e-08\n",
      "Epoch 780/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.2544e-08\n",
      "Epoch 781/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.2516e-08\n",
      "Epoch 782/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2488e-08\n",
      "Epoch 783/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1.2460e-08\n",
      "Epoch 784/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2432e-08\n",
      "Epoch 785/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2404e-08\n",
      "Epoch 786/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2377e-08\n",
      "Epoch 787/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.2349e-08\n",
      "Epoch 788/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.2322e-08\n",
      "Epoch 789/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.2294e-08\n",
      "Epoch 790/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.2267e-08\n",
      "Epoch 791/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.2240e-08\n",
      "Epoch 792/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2213e-08\n",
      "Epoch 793/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.2186e-08\n",
      "Epoch 794/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.2160e-08\n",
      "Epoch 795/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.2133e-08\n",
      "Epoch 796/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 1.2107e-08\n",
      "Epoch 797/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.2080e-08\n",
      "Epoch 798/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.2054e-08\n",
      "Epoch 799/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 1.2028e-08\n",
      "Epoch 800/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 1.2002e-08\n",
      "Epoch 801/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1976e-08\n",
      "Epoch 802/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.1950e-08\n",
      "Epoch 803/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1924e-08\n",
      "Epoch 804/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1898e-08\n",
      "Epoch 805/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.1873e-08\n",
      "Epoch 806/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1848e-08\n",
      "Epoch 807/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.1823e-08\n",
      "Epoch 808/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 1.1798e-08\n",
      "Epoch 809/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1772e-08\n",
      "Epoch 810/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 1.1747e-08\n",
      "Epoch 811/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1723e-08\n",
      "Epoch 812/1000\n",
      "48/48 [==============================] - 0s 754us/step - loss: 1.1698e-08\n",
      "Epoch 813/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.1673e-08\n",
      "Epoch 814/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1649e-08\n",
      "Epoch 815/1000\n",
      "48/48 [==============================] - 0s 798us/step - loss: 1.1624e-08\n",
      "Epoch 816/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1600e-08\n",
      "Epoch 817/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.1574e-08\n",
      "Epoch 818/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1550e-08\n",
      "Epoch 819/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.1526e-08\n",
      "Epoch 820/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1502e-08\n",
      "Epoch 821/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1478e-08\n",
      "Epoch 822/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.1454e-08\n",
      "Epoch 823/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1430e-08\n",
      "Epoch 824/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.1407e-08\n",
      "Epoch 825/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1383e-08\n",
      "Epoch 826/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.1360e-08\n",
      "Epoch 827/1000\n",
      "48/48 [==============================] - 0s 820us/step - loss: 1.1337e-08\n",
      "Epoch 828/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.1313e-08\n",
      "Epoch 829/1000\n",
      "48/48 [==============================] - 0s 799us/step - loss: 1.1290e-08\n",
      "Epoch 830/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1267e-08\n",
      "Epoch 831/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1244e-08\n",
      "Epoch 832/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 1.1222e-08\n",
      "Epoch 833/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1199e-08\n",
      "Epoch 834/1000\n",
      "48/48 [==============================] - 0s 769us/step - loss: 1.1176e-08\n",
      "Epoch 835/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.1154e-08\n",
      "Epoch 836/1000\n",
      "48/48 [==============================] - 0s 831us/step - loss: 1.1131e-08\n",
      "Epoch 837/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.1109e-08\n",
      "Epoch 838/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.1086e-08\n",
      "Epoch 839/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.1064e-08\n",
      "Epoch 840/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.1042e-08\n",
      "Epoch 841/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1.1020e-08\n",
      "Epoch 842/1000\n",
      "48/48 [==============================] - 0s 725us/step - loss: 1.0998e-08\n",
      "Epoch 843/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0976e-08\n",
      "Epoch 844/1000\n",
      "48/48 [==============================] - 0s 832us/step - loss: 1.0954e-08\n",
      "Epoch 845/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0933e-08\n",
      "Epoch 846/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 1.0911e-08\n",
      "Epoch 847/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.0889e-08\n",
      "Epoch 848/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0868e-08\n",
      "Epoch 849/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0846e-08\n",
      "Epoch 850/1000\n",
      "48/48 [==============================] - 0s 969us/step - loss: 1.0825e-08\n",
      "Epoch 851/1000\n",
      "48/48 [==============================] - 0s 2ms/step - loss: 1.0804e-08\n",
      "Epoch 852/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0783e-08\n",
      "Epoch 853/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0762e-08\n",
      "Epoch 854/1000\n",
      "48/48 [==============================] - 0s 1000us/step - loss: 1.0741e-08\n",
      "Epoch 855/1000\n",
      "48/48 [==============================] - 0s 937us/step - loss: 1.0720e-08\n",
      "Epoch 856/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 1.0699e-08\n",
      "Epoch 857/1000\n",
      "48/48 [==============================] - 0s 990us/step - loss: 1.0678e-08\n",
      "Epoch 858/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0658e-08\n",
      "Epoch 859/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 1.0637e-08\n",
      "Epoch 860/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0617e-08\n",
      "Epoch 861/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 1.0596e-08\n",
      "Epoch 862/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 1.0576e-08\n",
      "Epoch 863/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1.0556e-08\n",
      "Epoch 864/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.0536e-08\n",
      "Epoch 865/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0516e-08\n",
      "Epoch 866/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.0495e-08\n",
      "Epoch 867/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.0476e-08\n",
      "Epoch 868/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 1.0456e-08\n",
      "Epoch 869/1000\n",
      "48/48 [==============================] - 0s 852us/step - loss: 1.0436e-08\n",
      "Epoch 870/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.0416e-08\n",
      "Epoch 871/1000\n",
      "48/48 [==============================] - 0s 793us/step - loss: 1.0396e-08\n",
      "Epoch 872/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0377e-08\n",
      "Epoch 873/1000\n",
      "48/48 [==============================] - 0s 853us/step - loss: 1.0357e-08\n",
      "Epoch 874/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 1.0338e-08\n",
      "Epoch 875/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0318e-08\n",
      "Epoch 876/1000\n",
      "48/48 [==============================] - 0s 819us/step - loss: 1.0299e-08\n",
      "Epoch 877/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 1.0279e-08\n",
      "Epoch 878/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.0260e-08\n",
      "Epoch 879/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0241e-08\n",
      "Epoch 880/1000\n",
      "48/48 [==============================] - 0s 918us/step - loss: 1.0222e-08\n",
      "Epoch 881/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 1.0203e-08\n",
      "Epoch 882/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0184e-08\n",
      "Epoch 883/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.0165e-08\n",
      "Epoch 884/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.0146e-08\n",
      "Epoch 885/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1.0128e-08\n",
      "Epoch 886/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1.0109e-08\n",
      "Epoch 887/1000\n",
      "48/48 [==============================] - 0s 926us/step - loss: 1.0090e-08\n",
      "Epoch 888/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 1.0072e-08\n",
      "Epoch 889/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1.0054e-08\n",
      "Epoch 890/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1.0035e-08\n",
      "Epoch 891/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1.0017e-08\n",
      "Epoch 892/1000\n",
      "48/48 [==============================] - 0s 788us/step - loss: 9.9988e-09\n",
      "Epoch 893/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 9.9806e-09\n",
      "Epoch 894/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 9.9624e-09\n",
      "Epoch 895/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 9.9443e-09\n",
      "Epoch 896/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.9264e-09\n",
      "Epoch 897/1000\n",
      "48/48 [==============================] - 0s 757us/step - loss: 9.9085e-09\n",
      "Epoch 898/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.8907e-09\n",
      "Epoch 899/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.8728e-09\n",
      "Epoch 900/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 9.8551e-09\n",
      "Epoch 901/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.8375e-09\n",
      "Epoch 902/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.8198e-09\n",
      "Epoch 903/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 9.8023e-09\n",
      "Epoch 904/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.7848e-09\n",
      "Epoch 905/1000\n",
      "48/48 [==============================] - 0s 776us/step - loss: 9.7673e-09\n",
      "Epoch 906/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.7499e-09\n",
      "Epoch 907/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.7326e-09\n",
      "Epoch 908/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 9.7152e-09\n",
      "Epoch 909/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.6979e-09\n",
      "Epoch 910/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.6808e-09\n",
      "Epoch 911/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 9.6635e-09\n",
      "Epoch 912/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 9.6465e-09\n",
      "Epoch 913/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 9.6295e-09\n",
      "Epoch 914/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.6127e-09\n",
      "Epoch 915/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.5958e-09\n",
      "Epoch 916/1000\n",
      "48/48 [==============================] - 0s 768us/step - loss: 9.5789e-09\n",
      "Epoch 917/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.5620e-09\n",
      "Epoch 918/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.5453e-09\n",
      "Epoch 919/1000\n",
      "48/48 [==============================] - 0s 725us/step - loss: 9.5287e-09\n",
      "Epoch 920/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.5123e-09\n",
      "Epoch 921/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.4958e-09\n",
      "Epoch 922/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.4793e-09\n",
      "Epoch 923/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 9.4630e-09\n",
      "Epoch 924/1000\n",
      "48/48 [==============================] - 0s 734us/step - loss: 9.4465e-09\n",
      "Epoch 925/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 9.4301e-09\n",
      "Epoch 926/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.4138e-09\n",
      "Epoch 927/1000\n",
      "48/48 [==============================] - 0s 735us/step - loss: 9.3975e-09\n",
      "Epoch 928/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.3814e-09\n",
      "Epoch 929/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.3653e-09\n",
      "Epoch 930/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 9.3495e-09\n",
      "Epoch 931/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.3334e-09\n",
      "Epoch 932/1000\n",
      "48/48 [==============================] - 0s 815us/step - loss: 9.3174e-09\n",
      "Epoch 933/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.3016e-09\n",
      "Epoch 934/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 9.2857e-09\n",
      "Epoch 935/1000\n",
      "48/48 [==============================] - 0s 748us/step - loss: 9.2697e-09\n",
      "Epoch 936/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.2539e-09\n",
      "Epoch 937/1000\n",
      "48/48 [==============================] - 0s 746us/step - loss: 9.2384e-09\n",
      "Epoch 938/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 9.2228e-09\n",
      "Epoch 939/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 9.2073e-09\n",
      "Epoch 940/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 9.1918e-09\n",
      "Epoch 941/1000\n",
      "48/48 [==============================] - 0s 767us/step - loss: 9.1764e-09\n",
      "Epoch 942/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9.1609e-09\n",
      "Epoch 943/1000\n",
      "48/48 [==============================] - 0s 748us/step - loss: 9.1455e-09\n",
      "Epoch 944/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 9.1302e-09\n",
      "Epoch 945/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 9.1149e-09\n",
      "Epoch 946/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.0997e-09\n",
      "Epoch 947/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 9.0846e-09\n",
      "Epoch 948/1000\n",
      "48/48 [==============================] - 0s 731us/step - loss: 9.0694e-09\n",
      "Epoch 949/1000\n",
      "48/48 [==============================] - 0s 724us/step - loss: 9.0545e-09\n",
      "Epoch 950/1000\n",
      "48/48 [==============================] - 0s 723us/step - loss: 9.0394e-09\n",
      "Epoch 951/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 9.0246e-09\n",
      "Epoch 952/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 9.0097e-09\n",
      "Epoch 953/1000\n",
      "48/48 [==============================] - 0s 790us/step - loss: 8.9948e-09\n",
      "Epoch 954/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.9798e-09\n",
      "Epoch 955/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.9650e-09\n",
      "Epoch 956/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.9502e-09\n",
      "Epoch 957/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.9355e-09\n",
      "Epoch 958/1000\n",
      "48/48 [==============================] - 0s 841us/step - loss: 8.9209e-09\n",
      "Epoch 959/1000\n",
      "48/48 [==============================] - 0s 809us/step - loss: 8.9063e-09\n",
      "Epoch 960/1000\n",
      "48/48 [==============================] - 0s 852us/step - loss: 8.8917e-09\n",
      "Epoch 961/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 8.8773e-09\n",
      "Epoch 962/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.8629e-09\n",
      "Epoch 963/1000\n",
      "48/48 [==============================] - 0s 750us/step - loss: 8.8485e-09\n",
      "Epoch 964/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.8342e-09\n",
      "Epoch 965/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 8.8199e-09\n",
      "Epoch 966/1000\n",
      "48/48 [==============================] - 0s 745us/step - loss: 8.8057e-09\n",
      "Epoch 967/1000\n",
      "48/48 [==============================] - 0s 766us/step - loss: 8.7916e-09\n",
      "Epoch 968/1000\n",
      "48/48 [==============================] - 0s 756us/step - loss: 8.7772e-09\n",
      "Epoch 969/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 8.7631e-09\n",
      "Epoch 970/1000\n",
      "48/48 [==============================] - 0s 810us/step - loss: 8.7491e-09\n",
      "Epoch 971/1000\n",
      "48/48 [==============================] - 0s 936us/step - loss: 8.7349e-09\n",
      "Epoch 972/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 8.7209e-09\n",
      "Epoch 973/1000\n",
      "48/48 [==============================] - 0s 800us/step - loss: 8.7070e-09\n",
      "Epoch 974/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 8.6932e-09\n",
      "Epoch 975/1000\n",
      "48/48 [==============================] - 0s 777us/step - loss: 8.6793e-09\n",
      "Epoch 976/1000\n",
      "48/48 [==============================] - 0s 830us/step - loss: 8.6655e-09\n",
      "Epoch 977/1000\n",
      "48/48 [==============================] - 0s 872us/step - loss: 8.6517e-09\n",
      "Epoch 978/1000\n",
      "48/48 [==============================] - 0s 787us/step - loss: 8.6378e-09\n",
      "Epoch 979/1000\n",
      "48/48 [==============================] - 0s 851us/step - loss: 8.6242e-09\n",
      "Epoch 980/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.6106e-09\n",
      "Epoch 981/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.5970e-09\n",
      "Epoch 982/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.5835e-09\n",
      "Epoch 983/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.5700e-09\n",
      "Epoch 984/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.5559e-09\n",
      "Epoch 985/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.5420e-09\n",
      "Epoch 986/1000\n",
      "48/48 [==============================] - 0s 916us/step - loss: 8.5286e-09\n",
      "Epoch 987/1000\n",
      "48/48 [==============================] - 0s 958us/step - loss: 8.5152e-09\n",
      "Epoch 988/1000\n",
      "48/48 [==============================] - 0s 937us/step - loss: 8.5018e-09\n",
      "Epoch 989/1000\n",
      "48/48 [==============================] - 0s 894us/step - loss: 8.4886e-09\n",
      "Epoch 990/1000\n",
      "48/48 [==============================] - 0s 905us/step - loss: 8.4755e-09\n",
      "Epoch 991/1000\n",
      "48/48 [==============================] - 0s 957us/step - loss: 8.4623e-09\n",
      "Epoch 992/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.4491e-09\n",
      "Epoch 993/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.4360e-09\n",
      "Epoch 994/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.4229e-09\n",
      "Epoch 995/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.4099e-09\n",
      "Epoch 996/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.3968e-09\n",
      "Epoch 997/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.3839e-09\n",
      "Epoch 998/1000\n",
      "48/48 [==============================] - 0s 915us/step - loss: 8.3710e-09\n",
      "Epoch 999/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.3582e-09\n",
      "Epoch 1000/1000\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 8.3447e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ba0745a390>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = collaborative_model_clicks_functional(100)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "phone_data = train_clicks.to_numpy().T\n",
    "val_data = val_clicks.to_numpy().T\n",
    "\n",
    "print(phone_data.shape)\n",
    "model.fit(phone_data, train_clicks.to_numpy().T, epochs=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "data = [0 for i in range(96)]\n",
    "for i in range(52, 58):\n",
    "    data[i] = 1\n",
    "# for i in range(0, 6):\n",
    "#     data[i] = 1\n",
    "\n",
    "\n",
    "tes = np.array(data)\n",
    "tes = np.reshape(tes, (1,PHONE_COUNT))\n",
    "result = model.predict(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.50181411e-04 8.36646141e-05 1.38179894e-04 1.64047131e-04\n",
      "  2.12346466e-04 2.07178862e-04 2.55613500e-04 1.92830368e-04\n",
      "  2.08675468e-04 2.23822761e-04 3.04544228e-04 1.11448222e-04\n",
      "  1.51569984e-04 1.31440625e-04 1.26011364e-04 1.99253613e-04\n",
      "  1.31177309e-04 8.26737232e-05 4.06985724e-04 1.69450650e-04\n",
      "  1.25298568e-04 2.30456397e-04 1.97331727e-04 2.12345258e-04\n",
      "  1.78914313e-04 2.27233933e-04 2.18072120e-04 2.23911324e-04\n",
      "  2.15846492e-04 1.42526958e-04 9.75322037e-05 1.54418289e-04\n",
      "  2.59955123e-04 1.97970599e-04 1.06424908e-04 1.33048918e-04\n",
      "  2.28375342e-04 1.69202816e-04 1.69580453e-04 1.96613910e-04\n",
      "  1.24127677e-04 2.72897450e-04 1.01591475e-04 1.85154058e-04\n",
      "  1.19256358e-04 2.89799209e-04 1.05376006e-04 1.89187427e-04\n",
      "  1.56336595e-04 2.15953114e-04 3.99529294e-04 1.86528065e-04\n",
      "  9.99960124e-01 9.99969423e-01 9.99960780e-01 9.99960303e-01\n",
      "  9.99974310e-01 9.99966323e-01 2.56122672e-04 3.30494106e-04\n",
      "  2.18244968e-04 1.58670417e-04 2.63903785e-04 1.31612964e-04\n",
      "  7.65995355e-05 1.40750417e-04 1.42935940e-04 1.91892206e-04\n",
      "  1.54471461e-04 1.69164123e-04 1.65313846e-04 1.23619524e-04\n",
      "  1.36953007e-04 1.44193444e-04 1.25137245e-04 3.33171687e-04\n",
      "  1.71366191e-04 2.34408784e-04 2.20771763e-04 2.88213807e-04\n",
      "  1.57508795e-04 1.86347635e-04 1.49783984e-04 1.98852780e-04\n",
      "  1.27105566e-04 1.26595725e-04 1.63487915e-04 1.87712387e-04\n",
      "  1.51325206e-04 2.42595474e-04 1.32405854e-04 9.89077744e-05\n",
      "  1.12386821e-04 2.52661819e-04 1.60533120e-04 1.88743099e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Probability clicking 0.00015 for phone Oppo Reno 11 Pro id 1\n",
      "2. Probability clicking 0.00008 for phone Oppo Reno 11 id 2\n",
      "3. Probability clicking 0.00014 for phone Oppo Reno 11F id 3\n",
      "4. Probability clicking 0.00016 for phone Oppo Reno 10 id 4\n",
      "5. Probability clicking 0.00021 for phone Oppo Reno 10 Pro id 5\n",
      "6. Probability clicking 0.00021 for phone Oppo Reno 10 Pro+ id 6\n",
      "7. Probability clicking 0.00026 for phone Oppo Find X7 Ultra id 7\n",
      "8. Probability clicking 0.00019 for phone Oppo Find X7 id 8\n",
      "9. Probability clicking 0.00021 for phone Oppo A98 5G id 9\n",
      "10. Probability clicking 0.00022 for phone Oppo A60 id 10\n",
      "11. Probability clicking 0.00030 for phone Oppo Find N3 id 11\n",
      "12. Probability clicking 0.00011 for phone Oppo A79 id 12\n",
      "13. Probability clicking 0.00015 for phone Oppo A18 id 13\n",
      "14. Probability clicking 0.00013 for phone Oppo A38 id 14\n",
      "15. Probability clicking 0.00013 for phone Oppo Pad Neo id 15\n",
      "16. Probability clicking 0.00020 for phone Asus Zenfone 11 Ultra id 16\n",
      "17. Probability clicking 0.00013 for phone Asus Zenfone 10 id 17\n",
      "18. Probability clicking 0.00008 for phone Asus ROG Phone 8 Pro id 18\n",
      "19. Probability clicking 0.00041 for phone Asus ROG Phone 8 id 19\n",
      "20. Probability clicking 0.00017 for phone Infinix Smart 8 id 20\n",
      "21. Probability clicking 0.00013 for phone Infinix Smart 8 Pro id 21\n",
      "22. Probability clicking 0.00023 for phone Infinix GT 20 Pro id 22\n",
      "23. Probability clicking 0.00020 for phone Infinix Note 40 Pro id 23\n",
      "24. Probability clicking 0.00021 for phone Infinix Note 40 id 24\n",
      "25. Probability clicking 0.00018 for phone Infinix Hot 40i id 25\n",
      "26. Probability clicking 0.00023 for phone Infinix Hot 40 Pro id 26\n",
      "27. Probability clicking 0.00022 for phone Infinix Zero 30 5G id 27\n",
      "28. Probability clicking 0.00022 for phone Infinix Zero 30 4G id 28\n",
      "29. Probability clicking 0.00022 for phone Samsung Galaxy A15 id 29\n",
      "30. Probability clicking 0.00014 for phone Samsung Galaxy A25 id 30\n",
      "31. Probability clicking 0.00010 for phone Samsung Galaxy S24 Ultra id 31\n",
      "32. Probability clicking 0.00015 for phone Samsung Galaxy S24+ id 32\n",
      "33. Probability clicking 0.00026 for phone Samsung Galaxy S24 id 33\n",
      "34. Probability clicking 0.00020 for phone Samsung Galaxy Z Fold5 id 34\n",
      "35. Probability clicking 0.00011 for phone Samsung Galaxy Z Flip5 id 35\n",
      "36. Probability clicking 0.00013 for phone Samsung Galaxy M34 id 36\n",
      "37. Probability clicking 0.00023 for phone Samsung Galaxy M54 id 37\n",
      "38. Probability clicking 0.00017 for phone Samsung Galaxy A05s id 38\n",
      "39. Probability clicking 0.00017 for phone Samsung Galaxy A55 id 39\n",
      "40. Probability clicking 0.00020 for phone Samsung Galaxy A35 id 40\n",
      "41. Probability clicking 0.00012 for phone Vivo V30e id 41\n",
      "42. Probability clicking 0.00027 for phone Vivo V30 id 42\n",
      "43. Probability clicking 0.00010 for phone Vivo V30 Pro id 43\n",
      "44. Probability clicking 0.00019 for phone Vivo Y27s id 44\n",
      "45. Probability clicking 0.00012 for phone Vivo Y100 id 45\n",
      "46. Probability clicking 0.00029 for phone Vivo T3x id 46\n",
      "47. Probability clicking 0.00011 for phone Vivo Y18 id 47\n",
      "48. Probability clicking 0.00019 for phone Vivo V29e id 48\n",
      "49. Probability clicking 0.00016 for phone Vivo Y17s id 49\n",
      "50. Probability clicking 0.00022 for phone Vivo V29 id 50\n",
      "51. Probability clicking 0.00040 for phone Huawei Pura 70 Ultra id 51\n",
      "52. Probability clicking 0.00019 for phone Huawei Pura 70 Pro id 52\n",
      "53. Probability clicking 0.00026 for phone Huawei Nova 11 Pro id 59\n",
      "54. Probability clicking 0.00033 for phone Huawei Nova 11 id 60\n",
      "55. Probability clicking 0.00022 for phone iPhone 12 id 61\n",
      "56. Probability clicking 0.00016 for phone iPhone 13 id 62\n",
      "57. Probability clicking 0.00026 for phone iPhone SE (3rd gen) id 63\n",
      "58. Probability clicking 0.00013 for phone iPhone 14 id 64\n",
      "59. Probability clicking 0.00008 for phone iPhone 14 Plus id 65\n",
      "60. Probability clicking 0.00014 for phone iPhone 14 Pro id 66\n",
      "61. Probability clicking 0.00014 for phone iPhone 14 Pro Max id 67\n",
      "62. Probability clicking 0.00019 for phone iPhone 15 id 68\n",
      "63. Probability clicking 0.00015 for phone iPhone 15 Plus id 69\n",
      "64. Probability clicking 0.00017 for phone iPhone 15 Pro id 70\n",
      "65. Probability clicking 0.00017 for phone iPhone 15 Pro Max id 71\n",
      "66. Probability clicking 0.00012 for phone Realme C65 id 72\n",
      "67. Probability clicking 0.00014 for phone Realme C55 id 73\n",
      "68. Probability clicking 0.00014 for phone Realme C51 id 74\n",
      "69. Probability clicking 0.00013 for phone Realme C51s id 75\n",
      "70. Probability clicking 0.00033 for phone Realme Note 50 id 76\n",
      "71. Probability clicking 0.00017 for phone Realme C53 id 77\n",
      "72. Probability clicking 0.00023 for phone Realme C67 id 78\n",
      "73. Probability clicking 0.00022 for phone Realme 12 Pro+ 5G id 79\n",
      "74. Probability clicking 0.00029 for phone Realme 12+ 5G id 80\n",
      "75. Probability clicking 0.00016 for phone Realme 12 5G id 81\n",
      "76. Probability clicking 0.00019 for phone Realme 11 Pro+ 5G id 82\n",
      "77. Probability clicking 0.00015 for phone Xiaomi 14 id 83\n",
      "78. Probability clicking 0.00020 for phone Xiaomi 13T id 84\n",
      "79. Probability clicking 0.00013 for phone Redmi Note 13 Pro+ id 85\n",
      "80. Probability clicking 0.00013 for phone Redmi Note13 Pro 5G id 86\n",
      "81. Probability clicking 0.00016 for phone Redmi Note 13 Pro 4G id 87\n",
      "82. Probability clicking 0.00019 for phone Redmi Note 13 5G id 88\n",
      "83. Probability clicking 0.00015 for phone Redmi Note 13 4G id 89\n",
      "84. Probability clicking 0.00024 for phone Redmi 13C id 90\n",
      "85. Probability clicking 0.00013 for phone Redmi A3 id 91\n",
      "86. Probability clicking 0.00010 for phone Redmi 12 id 92\n",
      "87. Probability clicking 0.00011 for phone Poco F5 id 93\n",
      "88. Probability clicking 0.00025 for phone Poco X6 Pro 5G id 94\n",
      "89. Probability clicking 0.00016 for phone Poco X6 5G id 95\n",
      "90. Probability clicking 0.00019 for phone Poco C65 id 96\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "probability = result[check_user]\n",
    "data_user_clicked = data\n",
    "for i in range(PHONE_COUNT):\n",
    "    if data_user_clicked[i] != 1:\n",
    "        name =  df_phone_dataset.iloc[i]['name']\n",
    "        id =  df_phone_dataset.iloc[i]['id']\n",
    "        print(f'{index}. Probability clicking {probability[i]:0.5f} for phone { name } id { id }')\n",
    "        index += 1\n",
    "    else:\n",
    "        probability[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Recommending clicking 1.00 for phone Asus ROG Phone 8\n",
      "2. Recommending clicking 0.98 for phone Huawei Pura 70 Ultra\n",
      "3. Recommending clicking 0.82 for phone Realme Note 50\n",
      "4. Recommending clicking 0.81 for phone Huawei Nova 11\n",
      "5. Recommending clicking 0.75 for phone Oppo Find N3\n",
      "6. Recommending clicking 0.71 for phone Vivo T3x\n",
      "7. Recommending clicking 0.71 for phone Realme 12+ 5G\n",
      "8. Recommending clicking 0.67 for phone Vivo V30\n",
      "9. Recommending clicking 0.65 for phone iPhone SE (3rd gen)\n",
      "10. Recommending clicking 0.64 for phone Samsung Galaxy S24\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['probability'] = np.reshape(result[check_user], 96).astype(float)\n",
    "df_result['name'] = df_phone_dataset['name']\n",
    "sorted_df_result = df_result.sort_values(by='probability', ascending=False)\n",
    "# sorted_df_result = sorted_df_result[sorted_df_result['probability'] <= 0.7]\n",
    "sorted_df_result['probability'] = sorted_df_result['probability'] / sorted_df_result['probability'].max()\n",
    "datas = sorted_df_result.to_numpy()\n",
    "for i in range(10):\n",
    "    prob = datas[i][0]\n",
    "    print(f'{i + 1}. Recommending clicking {prob:0.2f} for phone { (datas[i][1]) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ROG STRIX\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    " model.save(\"model_C5.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
